import numpy
import matplotlib.pyplot as plt
import tensorflow
import h5py
import os


"""################ loading from hdf5 files #################"""

# Loading Dataset from Hdf5 file format

filename = "/mnt/d/spil3141's Datasets/Dataset 1.0/File1(4000 samples-post-padded).hdf5"
# filename = "D:\(Dataset 3.1)(2000 samples).hdf5"
def train_batch_generator():
    with h5py.File(filename, "r") as f:
        for X,Y in zip(f["/Dataset 3.1/Train/Feature"],f["/Dataset 3.1/Train/Label"]):
            #make sure sample and target type is a numpy array
            Y = tensorflow.one_hot(Y, depth=2)
            yield (X,Y)
def valid_batch_generator():
    with h5py.File(filename, "r") as f:
        for X,Y in zip(f["/Dataset 3.1/Valid/Feature"],f["/Dataset 3.1/Valid/Label"]):
            #make sure sample and target type is a numpy array
            Y = tensorflow.one_hot(Y, depth=2)
            yield (X,Y)
def test_batch_generator():
    with h5py.File(filename, "r") as f:
        for X,Y in zip(f["/Dataset 3.1/Test/Feature"],f["/Dataset 3.1/Test/Label"]):
            #make sure sample and target type is a numpy array
            Y = tensorflow.one_hot(Y, depth=2)
            yield (X,Y)

X_Train_data = tensorflow.data.Dataset.from_generator(train_batch_generator,
                                          output_types=('float32', 'int32'),
                                          output_shapes=(tensorflow.TensorShape((None,)), tensorflow.TensorShape((None,)))
                                         )
X_Valid_data = tensorflow.data.Dataset.from_generator(valid_batch_generator,
                                          output_types=('float32', 'int32'),
                                          output_shapes=(tensorflow.TensorShape((None,)), tensorflow.TensorShape((None,)))
                                         )
X_Test_data = tensorflow.data.Dataset.from_generator(test_batch_generator,
                                          output_types=('float32', 'int32'),
                                          output_shapes=(tensorflow.TensorShape((None,)), tensorflow.TensorShape((None,)))
                                         )

# Pre-processing 
def norm(x,_min,_max):
   return (x - _min)/(_max - _min)

def normalize_samples(feature,label):
   X = norm(feature,0,255)
   Y = label
   return X,Y
##Using the dataset batch function, shuffle and divide the dataset into batches
X_Train_data_norm = X_Train_data.map(normalize_samples)
X_Valid_data_norm = X_Valid_data.map(normalize_samples)
X_Test_data_norm = X_Test_data.map(normalize_samples)

# Batch split 
X_Train_data_norm = X_Train_data_norm.shuffle(5000).batch(2)
X_Valid_data_norm = X_Valid_data_norm.shuffle(5000).batch(2)
X_Test_data_norm = X_Test_data_norm.shuffle(5000).batch(2)

"""################ CNN Model (inceptionv3) #################"""
with h5py.File(filename, "r") as f:
   maxlen_pad = f["Dataset 3.1/Valid/Feature"][0].shape[0]
n_timesteps, n_features, n_input = int(maxlen_pad / 4), 4, maxlen_pad


def create_model():
    n_timesteps, n_features= int(maxlen_pad / 4), 4
    inputs = tensorflow.keras.layers.Input(shape=(maxlen_pad,))

    #Reshape the input dataset to fits the 1D CNN input requirements.
    x = tensorflow.keras.layers.Reshape((n_timesteps, n_features))(inputs)
    #x = tensorflow.keras.layers.BatchNormalization()(x)
    #1D CNN for 1D feature extractions
    x = tensorflow.keras.layers.Conv1D(64,64,strides=16, activation='relu')(x)
    x = tensorflow.keras.layers.MaxPooling1D(pool_size = 2)(x)
    x = tensorflow.keras.layers.Dropout(0.3)(x,training=True)
    x = tensorflow.keras.layers.Conv1D(32,5,strides=1, activation='relu')(x)
    x = tensorflow.keras.layers.MaxPooling1D(pool_size = 2)(x)
    x = tensorflow.keras.layers.Dropout(0.3)(x,training=True)
    x = tensorflow.keras.layers.Conv1D(16,5,strides=1, activation='relu')(x)
    x = tensorflow.keras.layers.MaxPooling1D(pool_size = 2)(x)
    x = tensorflow.keras.layers.Dropout(0.25)(x,training=True)
    x = tensorflow.keras.layers.Flatten()(x)
    x = tensorflow.keras.layers.Dense(264, activation='relu')(x)
    x = tensorflow.keras.layers.Dropout(rate=0.3)(x, training=True) 
    x = tensorflow.keras.layers.Dense(23, activation='relu')(x)
    x = tensorflow.keras.layers.Dropout(rate=0.3)(x, training=True)
    outputs = tensorflow.keras.layers.Dense(2, activation='softmax')(x)
    model = tensorflow.keras.models.Model(inputs=inputs, outputs=outputs)
    return model

model = create_model()
model.summary()
print("Input Shapes: ")
for i in model.layers:
    print(i.name,"\t" ,i.input_shape)

# Define the checkpoint directory to store the checkpoints

checkpoint_dir = 'training_checkpoints'
# Name of the checkpoint files
checkpoint_prefix = os.path.join(checkpoint_dir, "ckpt_{epoch}")

# Function for decaying the learning rate.
# You can define any decay function you need.
def decay(epoch):
  if epoch < 3:
    return 1e-3
  elif epoch >= 3 and epoch < 7:
    return 1e-4
  else:
    return 1e-5

# Callback for printing the LR at the end of each epoch.
class PrintLR(tensorflow.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs=None):
    print('\nLearning rate for epoch {} is {}'.format(epoch + 1,
                                                      model.optimizer.lr.numpy()))

callbacks = [
    tensorflow.keras.callbacks.TensorBoard(log_dir='logs'),
    tensorflow.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,
                                       save_weights_only=True),
    tensorflow.keras.callbacks.LearningRateScheduler(decay),
    PrintLR()
]

opt = tensorflow.keras.optimizers.Adam(learning_rate=0.001)
model.compile(loss='categorical_crossentropy', 
                optimizer=opt, metrics=['accuracy']) 


history = model.fit(X_Train_data_norm,
                    epochs=10,
                    verbose=1,
                    validation_data = X_Valid_data_norm,
                    callbacks= callbacks
                   )


