"""#### Importting library ###"""

import numpy
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Activation, Reshape, LSTM, Bidirectional
from tensorflow.keras.layers import Embedding, Reshape, TimeDistributed
from tensorflow.keras.layers import Flatten,Conv1D, GlobalMaxPooling1D,MaxPooling1D,GlobalAveragePooling1D
import tensorflow as tf
from tensorflow.keras import layers
from sklearn.metrics import confusion_matrix
import tensorflow
import pandas as pd
from datetime import datetime
from sklearn.preprocessing import StandardScaler
import h5py

maxlen_pad = 6111324
EPOCHS = None
Batch_size = None
"""################ loading from file #################"""

# print("Loading Data")
# data = numpy.load("Dataset 2.0 (completed)/(20000xunknow)(1D Data Section Extraction-padded).npz")
# X_train= data['X_train']
# y_train= data['y_train']
# X_test = data["X_test"]
# y_test = data["y_test"]

# data = numpy.load("(20000x110000)(1D Data Section Extraction-padded).npz")
# X_train= data['X_train']
# y_train= data['y_train']
# X_test = data["X_test"]
# y_test = data["y_test"]
# X_valid = data['X_valid']
# y_valid = data['y_valid']
# print("Done")
# print("valid Dataset Size : ", X_valid.shape)
# print("Training Dataset Size : ", X_train.shape)      
# print("Test Dataset Size : ", X_test.shape)

# Loading Dataset from Hdf5 file format

filename = "D:/Datasets/(Dataset 3.1).hdf5"

def train_batch_generator():
    with h5py.File(filename, "r") as f:
        for X,Y in zip(f["/Dataset 3.1/Train/Feature"],f["/Dataset 3.1/Train/Label"]):
            #make sure sample and target type is a numpy array
            Y = tf.one_hot(Y, depth=2)
            yield (X,Y)
def valid_batch_generator():
    with h5py.File(filename, "r") as f:
        for X,Y in zip(f["/Dataset 3.1/Valid/Feature"],f["/Dataset 3.1/Valid/Label"]):
            #make sure sample and target type is a numpy array
            Y = tf.one_hot(Y, depth=2)
            yield (X,Y)
def test_batch_generator():
    with h5py.File(filename, "r") as f:
        for X,Y in zip(f["/Dataset 3.1/Test/Feature"],f["/Dataset 3.1/Test/Label"]):
            #make sure sample and target type is a numpy array
            Y = tf.one_hot(Y, depth=2)
            yield (X,Y)

X_Train_data = tf.data.Dataset.from_generator(train_batch_generator,
                                          output_types=('int32', 'int32'),
                                          output_shapes=(tf.TensorShape((None,)), tf.TensorShape((None,)))
                                         )
X_Valid_data = tf.data.Dataset.from_generator(valid_batch_generator,
                                          output_types=('int32', 'int32'),
                                          output_shapes=(tf.TensorShape((None,)), tf.TensorShape((None,)))
                                         )
X_Test_data = tf.data.Dataset.from_generator(test_batch_generator,
                                          output_types=('int32', 'int32'),
                                          output_shapes=(tf.TensorShape((None,)), tf.TensorShape((None,)))
                                         )


## Using dataset map function we need to scale the dataset 

##Using the dataset batch function, shuffle and divide the dataset into batches
X_Train_data = X_Train_data.shuffle(5000).batch(10)
X_Valid_data = X_Valid_data.shuffle(5000).batch(10)
X_Test_data = X_Test_data.shuffle(5000).batch(10)

# for count_batch in X_Train_data.repeat().take(2):
#   print((count_batch[0]))
# for count_batch in X_Valid_data.repeat().take(2):
#   print((count_batch[0]))
# for count_batch in X_Test_data.repeat().take(2):
#   print((count_batch[0]))
print("Done")          



#standardization
# stdsc = StandardScaler()
# X_train_std = stdsc.fit_transform(X_train)
# X_valid_std = stdsc.transform(X_valid)
# X_test_std = stdsc.transform(X_test)

# """### One-Hot Encoding of classes ###"""
# y_train_onehot = tf.keras.utils.to_categorical(y_train)
# y_valid_onehot = tf.keras.utils.to_categorical(y_valid)
# y_test_onehot = tf.keras.utils.to_categorical(y_test)

# for i,j in enumerate(X_train):
#     X_train[i] = numpy.asarray(j,dtype=numpy.int32)
#     # X_valid[i] = numpy.asarray(j,dtype=numpy.int32)
#     print(i,type(j))
# y_train_onehot = tf.keras.utils.to_categorical(y_train)
# y_valid_onehot = tf.keras.utils.to_categorical(y_valid)
# print(y_train_onehot.shape)
# print(y_valid_onehot.shape)

"""############################  model #####################################"""


EPOCHS = 10
Batch_size = 10

def create_model1():
    print('Build model...')
    model = Sequential()
    n_timesteps, n_features, n_input = int(30000 / 4), 4, 30000 
    model.add(Reshape((n_timesteps, n_features), input_shape=(n_input,)))
    # A Convolution1D layer
    model.add(Conv1D(128,#filters (number of features))
                      32,#kernel size
                      padding='valid',
                      activation='relu',
                      strides=1))
    model.add(LSTM(32))
    model.add(layers.Dense(1, activation='sigmoid'))
    return model 

def create_model2():
    print('Build model...')
    model = Sequential()
    n_timesteps, n_features, n_input = int(30000 / 8), 8, 30000
    model.add(Reshape((n_timesteps, n_features), input_shape=(n_input,)))
    model.add(layers.Conv1D(32, 5, activation='relu',
                            input_shape=(None, n_input)))
    model.add(layers.MaxPooling1D(3))
    model.add(Dropout(0.2))
    model.add(layers.Conv1D(32, 5, activation='relu'))
    model.add(Bidirectional(LSTM(32)))
    model.add(layers.Dense(164, activation='relu'))
    model.add(layers.Dropout(0.5))
    # model.add(layers.Dense(1, activation='sigmoid'))
    model.add(layers.Dense(2, activation='softmax'))
    return model 
def create_Testmodel():
    print('Build model...')
    model = Sequential()
    n_timesteps, n_features, n_input = int(maxlen_pad / 4), 4, maxlen_pad
    model.add(Reshape((n_timesteps, n_features), input_shape=(n_input,)))
    #model.add(layers.Embedding(256,64,input_length=X_train.shape[1]))
    model.add(layers.Conv1D(32,5, activation='relu'))
    model.add(layers.BatchNormalization())
    model.add(layers.MaxPooling1D(5))
    model.add(layers.Dropout(0.3))
    model.add(layers.Conv1D(32,5, activation='relu'))
    model.add(layers.BatchNormalization())
    model.add(layers.MaxPooling1D(5))
    model.add(layers.Dropout(0.3))
    model.add(Bidirectional(LSTM(16,  return_sequences=True)))
    model.add(Bidirectional(LSTM(8)))
    #model.add(layers.Flatten())
    model.add(layers.Dense(100, activation='relu'))
    model.add(layers.Dropout(0.3))
    #model.add(layers.Dense(1, activation='sigmoid'))
    model.add(layers.Dense(2, activation='softmax'))
    return model 


# model = create_model2()
model = create_Testmodel()

model.summary()

# opt = tensorflow.keras.optimizers.Adam(learning_rate=0.001)
# model.compile(loss='binary_crossentropy', 
#               optimizer=opt, metrics=['acc']) 

model.compile(optimizer= tf.keras.optimizers.SGD(learning_rate=0.01, #default = 0.01
                                                         momentum=0.0,
                                                         nesterov=False),
                      loss="categorical_crossentropy",
                      metrics = ["accuracy"])  

## Fitting Function 

# history = model.fit(X_train_std, y_train_onehot, 
#                     batch_size= Batch_size, epochs=EPOCHS,
#                     validation_data = (X_valid_std,y_valid_onehot)
#                     )
history = model.fit(X_Train_data,
                    epochs=EPOCHS,
                    verbose = 1
                    )

history = history.history
        
"""### Saving history ###"""
hist_df = pd.DataFrame(history)
# or save to csv:
hist_csv_file = 'history.csv'
with open(hist_csv_file, mode='w') as f:
    hist_df.to_csv(f)
# load history
history = pd.read_csv("history.csv")

""" Saving Model """
# # Saving Model
# model.save("Saved_Model_" + datetime.now().strftime("%Y%m%d-%H%M%S"))
# #Loading
# loaded_model = tf.keras.models.load_model('Saved_Model_20200515-183140')

# opt = tensorflow.keras.optimizers.Adam(learning_rate=0.001)
# loaded_model.compile(loss='binary_crossentropy', 
#               optimizer=opt, metrics=['acc']) 

"""######################## Evaluation ######################"""
#Loss
 

epochs = numpy.arange(1, EPOCHS + 1)
plt.plot(epochs, history['loss'],label = "Train_loss")
plt.plot(epochs, history['val_loss'], label = "Valid_loss" )
plt.xlabel('epochs')
plt.ylabel('loss')
plt.legend()
plt.show()

#Accuracy
plt.plot(epochs, history['accuracy'],label = "Train_acc")
plt.plot(epochs, history['val_accuracy'],label = "valid_Acc")
plt.legend()
plt.xlabel('epochs')
plt.ylabel('accuracy')
plt.show()

#Evaluating
# train_loss, train_acc = model.evaluate(X_train, y_train,verbose = 1)
# valid_loss, valid_acc = loaded_model.evaluate(X_valid, y_valid,verbose = 1)
test_loss, test_acc = model.evaluate(X_test_std, y_test_onehot,verbose = 0)
# print("On Training Set: (loss,acc): %.5f %.5f" % (train_loss,train_acc))
# print("On Validation Set: (loss,acc): %.5f %.5f " % (valid_loss, valid_acc))
print("On Test Set: (loss,acc): %.5f %.5f" % (test_loss,test_acc))
# # More evaluation
X_test_temp = X_test.astype("float32")
y_true = y_test
result = model.predict(X_test_temp)
y_pred = [numpy.argmax(i) for i in result]
cm = confusion_matrix(y_true, y_pred)
precision =  cm[0,0]/(cm[0,0] + cm[0,1])
recall = cm[0,0]/(cm[0,0] + cm[1,0])
F1score = 2 * ((precision * recall) / (precision + recall))
#F1score = f1_score(y_true,y_pred)
print("Precision: ", precision)
print("Recall: ", recall)
print("F1_Score: ", F1score)