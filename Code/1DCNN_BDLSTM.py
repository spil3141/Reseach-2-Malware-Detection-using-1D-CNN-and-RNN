"""#### Importting library ###"""

import numpy
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Activation, Reshape, LSTM, Bidirectional
from tensorflow.keras.layers import Embedding, Reshape, TimeDistributed
from tensorflow.keras.layers import Flatten,Conv1D, GlobalMaxPooling1D,MaxPooling1D,GlobalAveragePooling1D
import tensorflow as tf
from tensorflow.keras import layers
from sklearn.metrics import confusion_matrix
import tensorflow
import pandas as pd
from datetime import datetime
from sklearn.preprocessing import StandardScaler
import h5py
import numpy
import os
from sklearn.metrics import f1_score
from sklearn.metrics import precision_recall_fscore_support

"""################ loading from file #################"""

# print("Loading Data")
# data = numpy.load("Dataset 2.0 (completed)/(20000xunknow)(1D Data Section Extraction-padded).npz")
# X_train= data['X_train']
# y_train= data['y_train']
# X_test = data["X_test"]
# y_test = data["y_test"]

# data = numpy.load("(20000x110000)(1D Data Section Extraction-padded).npz")
# X_train= data['X_train']
# y_train= data['y_train']
# X_test = data["X_test"]
# y_test = data["y_test"]
# X_valid = data['X_valid']
# y_valid = data['y_valid']
# print("Done")
# print("valid Dataset Size : ", X_valid.shape)
# print("Training Dataset Size : ", X_train.shape)      
# print("Test Dataset Size : ", X_test.shape)

# Loading Dataset from Hdf5 file format

filename = "D:/Dataset 2.0 (completed)/Dataset-3-1-4000-samples.hdf5"

def train_batch_generator():
    with h5py.File(filename, "r") as f:
        for X,Y in zip(f["/Dataset 3.1/Train/Feature"],f["/Dataset 3.1/Train/Label"]):
            #make sure sample and target type is a numpy array
            Y = tf.one_hot(Y, depth=2)
            yield (X,Y)
def valid_batch_generator():
    with h5py.File(filename, "r") as f:
        for X,Y in zip(f["/Dataset 3.1/Valid/Feature"],f["/Dataset 3.1/Valid/Label"]):
            #make sure sample and target type is a numpy array
            Y = tf.one_hot(Y, depth=2)
            yield (X,Y)
def test_batch_generator():
    with h5py.File(filename, "r") as f:
        for X,Y in zip(f["/Dataset 3.1/Test/Feature"],f["/Dataset 3.1/Test/Label"]):
            #make sure sample and target type is a numpy array
            Y = tf.one_hot(Y, depth=2)
            yield (X,Y)

X_Train_data = tf.data.Dataset.from_generator(train_batch_generator,
                                          output_types=('float32', 'int32'),
                                          output_shapes=(tf.TensorShape((None,)), tf.TensorShape((None,)))
                                         )
X_Valid_data = tf.data.Dataset.from_generator(valid_batch_generator,
                                          output_types=('float32', 'int32'),
                                          output_shapes=(tf.TensorShape((None,)), tf.TensorShape((None,)))
                                         )
X_Test_data = tf.data.Dataset.from_generator(test_batch_generator,
                                          output_types=('float32', 'int32'),
                                          output_shapes=(tf.TensorShape((None,)), tf.TensorShape((None,)))
                                         )


import time

        
def benchmark(dataset, num_epochs=2):
    start_time = time.perf_counter()
    for epoch_num in range(num_epochs):
        for sample in dataset:
            # Performing a training step
            time.sleep(0.01)
    tf.print("Execution time:", time.perf_counter() - start_time)



## Visualizing dataset batches 
def plot_batch_sizes(ds,name):
  batch_sizes = [(numpy.asarray(batch)).shape[0] for batch in ds]
  plt.bar(range(len(batch_sizes)), batch_sizes)
  plt.xlabel('Batch number')
  plt.ylabel('Batch size')
  plt.title(name)
  
#plot_batch_sizes(X_Test_data,"Test Dataset")  

# Pre-processing 
def norm(x,_min,_max):
   return (x - _min)/(_max - _min)

def normalize_samples(feature,label):
   X = norm(feature,0,255)
   Y = label
   return X,Y
##Using the dataset batch function, shuffle and divide the dataset into batches
X_Train_data_norm = X_Train_data.map(normalize_samples)
X_Valid_data_norm = X_Valid_data.map(normalize_samples)
X_Test_data_norm = X_Test_data.map(normalize_samples)

# Batch split 
X_Train_data_norm = X_Train_data_norm.shuffle(5000).batch(10)
X_Valid_data_norm = X_Valid_data_norm.shuffle(5000).batch(10)
X_Test_data_norm = X_Test_data_norm.shuffle(5000).batch(10)


#benchmark(X_Test_data_norm)
#benchmark((X_Test_data_norm).prefetch(tf.data.experimental.AUTOTUNE))

#benchmark(tf.data.Dataset.range(2).interleave(X_Test_data,num_parallel_calls=tf.data.experimental.AUTOTUNE))

print("Done")          



#standardization
# stdsc = StandardScaler()
# X_train_std = stdsc.fit_transform(X_train)
# X_valid_std = stdsc.transform(X_valid)
# X_test_std = stdsc.transform(X_test)

# """### One-Hot Encoding of classes ###"""
# y_train_onehot = tf.keras.utils.to_categorical(y_train)
# y_valid_onehot = tf.keras.utils.to_categorical(y_valid)
# y_test_onehot = tf.keras.utils.to_categorical(y_test)

# for i,j in enumerate(X_train):
#     X_train[i] = numpy.asarray(j,dtype=numpy.int32)
#     # X_valid[i] = numpy.asarray(j,dtype=numpy.int32)
#     print(i,type(j))
# y_train_onehot = tf.keras.utils.to_categorical(y_train)
# y_valid_onehot = tf.keras.utils.to_categorical(y_valid)
# print(y_train_onehot.shape)
# print(y_valid_onehot.shape)

"""############################  model #####################################"""


EPOCHS = 10
Batch_size = None
#size = next(valid_batch_generator())
#maxlen_pad = size[0].shape[0]
with h5py.File(filename, "r") as f:
   maxlen_pad = f["Dataset 3.1/Valid/Feature"][0].shape[0]

def create_model1():
    print('Build model...')
    model = Sequential()
    #n_timesteps, n_features, n_input = int(X_train.shape[1] / 8), 8, X_train.shape[1]
    #model.add(Reshape((n_timesteps, n_features), input_shape=(n_input,)))
    # A Convolution1D layer
    model.add(Conv1D(128,#filters (number of features))
                      32,#kernel size
                      padding='valid',
                      activation='relu',
                      strides=1))
    model.add(LSTM(32))
    model.add(layers.Dense(1, activation='sigmoid'))
    return model 

def create_model2():
    print('Build model...')
    model = Sequential()
    n_timesteps, n_features, n_input = int(X_train.shape[1] / 8), 8, X_train.shape[1]
    model.add(Reshape((n_timesteps, n_features), input_shape=(n_input,)))
    model.add(layers.Conv1D(32, 5, activation='relu',
                            input_shape=(None, n_input)))
    model.add(layers.MaxPooling1D(3))
    model.add(Dropout(0.2))
    model.add(layers.Conv1D(32, 5, activation='relu'))
    model.add(Bidirectional(LSTM(32)))
    model.add(layers.Dense(164, activation='relu'))
    model.add(layers.Dropout(0.5))
    # model.add(layers.Dense(1, activation='sigmoid'))
    model.add(layers.Dense(2, activation='softmax'))
    return model 
def create_model3():
    print('Build model...')
    #junk layers
    #model.add(Reshape((n_timesteps, n_features), input_shape=(n_input,)))
    #model.add(layers.Embedding(256,64,input_length=X_train.shape[1]))
    #model.add(layers.Conv1D(32,5, activation='relu'))
    #model.add(layers.BatchNormalization())
    #model.add(layers.MaxPooling1D(5))
    #model.add(layers.Dropout(0.3))
    #model.add(Bidirectional(LSTM(16,  return_sequences=True)))
    #model.add(layers.Flatten())
    #model.add(layers.Dense(2, activation='softmax'))
    #model.add(layers.Dense(2, activation='sigmoid'))
    #Layers
    
    model = Sequential()
    
    #Reshape the input dataset to fits the 1D CNN input requirements.
    n_timesteps, n_features, n_input = int(maxlen_pad / 4), 4, maxlen_pad
    model.add(Reshape((n_timesteps, n_features), input_shape=(n_input,)))
    
    #Applies Scaling to the output of the reshape layer. 
    model.add(layers.BatchNormalization())
    
    #1D CNN for 1D feature extractions
    model.add(layers.Conv1D(64,64,strides=16, activation='relu'))
    model.add(layers.MaxPooling1D(pool_size = 2))
    model.add(layers.Dropout(0.3))
    
    #Bi-D LSTM layer for sequential data learning 
    model.add(Bidirectional(LSTM(16)))
    
    #Dense Layer for generalization 
    model.add(layers.Dense(100, activation='relu'))
    model.add(layers.Dropout(0.5))
    
    #Last Dense layer for Reducing the dimensional for getting the needed classes. 
    model.add(layers.Dense(2, activation='softmax'))
    return model 
 
def create_Test_Model():
    model = Sequential()
    #Reshape the input dataset to fits the 1D CNN input requirements.
    n_timesteps, n_features, n_input = int(maxlen_pad / 4), 4, maxlen_pad
    model.add(Reshape((n_timesteps, n_features), input_shape=(maxlen_pad,)))
    
    #Applies Scaling to the output of the reshape layer. 
    model.add(layers.BatchNormalization())
    
    #1D CNN for 1D feature extractions
    model.add(layers.Conv1D(64,64,strides=16, activation='relu'))
    model.add(layers.MaxPooling1D(pool_size = 2))
    model.add(layers.Dropout(0.3))
    model.add(layers.Conv1D(128,5,strides=1, activation='relu'))
    model.add(layers.MaxPooling1D(pool_size = 2))
    model.add(layers.Dropout(0.3))
    model.add(layers.Conv1D(256,5,strides=1, activation='relu'))
    model.add(layers.MaxPooling1D(pool_size = 2))
    model.add(layers.Dropout(0.25))
    
    #Bi-D LSTM layer for sequential data learning 
    model.add(Bidirectional(LSTM(256,  return_sequences=True)))
    model.add(Bidirectional(LSTM(128,  return_sequences=True)))
    model.add(Bidirectional(LSTM(64)))
    
    #Dense Layer for generalization 
    model.add(layers.Dense(64, activation='relu'))
    model.add(layers.Dropout(0.3))
    
    #Last Dense layer for Reducing the dimensional for getting the needed classes. 
    model.add(layers.Dense(2, activation='softmax'))
    return model 

"""###################            Distributed Learning         #######################"""

#
#strategy = tf.distribute.MirroredStrategy()
#print('Number of devices: {}'.format(strategy.num_replicas_in_sync))
#
#n_timesteps, n_features, n_input = int(maxlen_pad / 4), 4, maxlen_pad
#
#with strategy.scope():
#    model = tf.keras.Sequential()
#    #Reshape the input dataset to fits the 1D CNN input requirements.
#    model.add(Reshape((n_timesteps, n_features), input_shape=(maxlen_pad,)))
#
#    #Applies Scaling to the output of the reshape layer. 
#    model.add(layers.BatchNormalization())
#
#    #1D CNN for 1D feature extractions
#    model.add(layers.Conv1D(64,64,strides=16, activation='relu'))
#    model.add(layers.MaxPooling1D(pool_size = 2))
#    model.add(layers.Dropout(0.3))
#
#    #Bi-D LSTM layer for sequential data learning 
#    model.add(Bidirectional(LSTM(16)))
#
#    #Dense Layer for generalization 
#    model.add(layers.Dense(100, activation='relu'))
#    model.add(layers.Dropout(0.5))
#
#    #Last Dense layer for Reducing the dimensional for getting the needed classes. 
#    model.add(layers.Dense(2, activation='softmax'))
#    

# Define the checkpoint directory to store the checkpoints

checkpoint_dir = 'training_checkpoints'
# Name of the checkpoint files
checkpoint_prefix = os.path.join(checkpoint_dir, "ckpt_{epoch}")

# Function for decaying the learning rate.
# You can define any decay function you need.
def decay(epoch):
  if epoch < 3:
    return 1e-3
  elif epoch >= 3 and epoch < 7:
    return 1e-4
  else:
    return 1e-5

# Callback for printing the LR at the end of each epoch.
class PrintLR(tf.keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs=None):
    print('\nLearning rate for epoch {} is {}'.format(epoch + 1,
                                                      model.optimizer.lr.numpy()))

callbacks = [
    tf.keras.callbacks.TensorBoard(log_dir='logs'),
    tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_prefix,
                                       save_weights_only=True),
    tf.keras.callbacks.LearningRateScheduler(decay),
    PrintLR()
]

    

#model = test()
model = create_model3()
model.summary()
print("Input Shapes: ")
for i in model.layers:
    print(i.name,"\t" ,i.input_shape)

""" Compiling mode"""
#opt = tensorflow.keras.optimizers.Adam(learning_rate=0.001)
#model.compile(loss='binary_crossentropy', 
#               optimizer=opt, metrics=['accuracy']) 

#model.compile(optimizer='adam', loss='mae')

#model.compile(optimizer='adam',
#                  loss='categorical_crossentropy',
#                  metrics=['accuracy'])
model.compile(optimizer= tf.keras.optimizers.SGD(learning_rate=0.01, #default = 0.01
                                                 momentum=0.9,
                                                 decay=1e-2,
                                                 nesterov=False),
                      loss="categorical_crossentropy",
                      metrics = ["accuracy"])   

#model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),
#            optimizer=tf.keras.optimizers.Adam(),
#            metrics=['accuracy'])

## Fitting Function 

# Fit with tf.data """
history = model.fit(X_Train_data_norm,
                    epochs=EPOCHS,
                    callbacks= callbacks,
                    verbose=1,
                    validation_data = X_Valid_data_norm
                   )
"""### Saving history ###"""
hist_df = pd.DataFrame(history)
# or save to csv:
hist_csv_file = 'history.csv'
with open(hist_csv_file, mode='w') as f:
    hist_df.to_csv(f)

# Fit with generator """
#history = model.fit(train_batch_generator,
#                    epochs=EPOCHS,
#                    verbose=1,
#                    validation_data = valid_batch_generator
#                   )

# Fit Normal
#callback = tf.keras.callbacks.EarlyStopping(monitor="accuracy",patience=3)
#history = model.fit(X_train, y_train_onehot, 
#                    batch_size= Batch_size, epochs=EPOCHS,
#                    validation_data = (X_valid,y_valid_onehot),
#                    callbacks=[callback],
#                    )

# load history
history = pd.read_csv("history.csv")
#load model weights
model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))
#$ tensorboard --logdir=path/to/log-directory


"""######################## Evaluation ######################"""
#Getting predicted values on test dataset from model
result = model.predict(X_Test_data_norm,verbose=1)
y_pred = [numpy.argmax(i) for i in result]
y_pred = numpy.asarray(y_pred)
#calculate the true label for test dataset
y_true = []
for i in X_Test_data_norm.unbatch():
    y_true.append(numpy.argmax(i[1].numpy()))
y_true = numpy.asarray(y_true)

#find the confusion matrix
cm = confusion_matrix(y_true, y_pred)
#precision =  cm[0,0]/(cm[0,0] + cm[0,1])
#recall = cm[0,0]/(cm[0,0] + cm[1,0])
temp = precision_recall_fscore_support(y_true, y_pred, average='binary')
precision = temp[0]
recall = temp[1]
F1score = temp[2]
#F1score = f1_score(y_true, y_pred, average='weighted')
#F1score = 2 * ((precision * recall) / (precision + recall))

#Loss
epochs = numpy.arange(1, EPOCHS + 1)
plt.plot(epochs, history.history['loss'],label = "Train_loss")
plt.plot(epochs, history.history['val_loss'], label = "Valid_loss" )
plt.xlabel('epochs')
plt.ylabel('loss')
plt.legend()
plt.show()

#Accuracy
plt.plot(epochs, history.history['accuracy'],label = "Train_acc")
plt.plot(epochs, history.history['val_accuracy'],label = "valid_Acc")
plt.legend()
plt.xlabel('epochs')
plt.ylabel('accuracy')
plt.show()

test_loss, test_acc = model.evaluate(X_Test_data_norm,verbose = 1)
print("On Test Set: (loss,acc): %.5f %.5f" % (test_loss,test_acc))
#F1score = f1_score(y_true,y_pred)
print("")
print("Precision: ", precision)
print("Recall: ", recall)
print("F1_Score: ", F1score)
print("confusion matrix:")
print("")
print(cm)


