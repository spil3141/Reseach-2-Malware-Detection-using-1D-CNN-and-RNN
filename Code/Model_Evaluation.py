"""#### Setup  ###"""

import numpy
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Activation, Reshape, LSTM, Bidirectional
from tensorflow.keras.layers import Embedding, Reshape, TimeDistributed
from tensorflow.keras.layers import Flatten,Conv1D, GlobalMaxPooling1D,MaxPooling1D,GlobalAveragePooling1D
import tensorflow as tf
from tensorflow.keras import layers
from sklearn.metrics import confusion_matrix
import tensorflow
import pandas as pd
from datetime import datetime
from sklearn.preprocessing import StandardScaler
import h5py
import numpy
import os
from sklearn.metrics import f1_score
from sklearn.metrics import precision_recall_fscore_support
import time
from sklearn.metrics import accuracy_score

"""################ loading from file #################"""
# Loading Dataset from Hdf5 file format

filename = "/mnt/d/Dataset 2.0 (completed)/(Dataset 3.1)(2000 samples).hdf5"

def test_batch_generator():
    with h5py.File(filename, "r") as f:
        for X,Y in zip(f["/Dataset 3.1/Train/Feature"],f["/Dataset 3.1/Train/Label"]):
            #make sure sample and target type is a numpy array
            Y = tf.one_hot(Y, depth=2)
            yield (X,Y)


X_Test_data = tf.data.Dataset.from_generator(test_batch_generator,
                                          output_types=('float32', 'int32'),
                                          output_shapes=(tf.TensorShape((None,)), tf.TensorShape((None,)))
                                         )


#SEttin up model 
checkpoint_dir = './training_checkpoints'
with h5py.File(filename, "r") as f:
   maxlen_pad = f["Dataset 3.1/Valid/Feature"][0].shape[0]
n_timesteps, n_features, n_input = int(maxlen_pad / 4), 4, maxlen_pad
strategy = tf.distribute.MirroredStrategy()
print('Number of devices: {}'.format(strategy.num_replicas_in_sync))
with strategy.scope():
    model = tf.keras.Sequential()
    #Reshape the input dataset to fits the 1D CNN input requirements.
    n_timesteps, n_features, n_input = int(maxlen_pad / 4), 4, maxlen_pad
    model.add(Reshape((n_timesteps, n_features), input_shape=(maxlen_pad,)))
    
    #Applies Scaling to the output of the reshape layer. 
    #model.add(layers.BatchNormalization())
    
    #1D CNN for 1D feature extractions
    model.add(layers.Conv1D(64,64,strides=16, activation='relu'))
    model.add(layers.MaxPooling1D(pool_size = 2))
    model.add(layers.Dropout(0.3,training=False))
    
    #Bi-D LSTM layer for sequential data learning 
    model.add(Bidirectional(LSTM(16)))
    
    #Dense Layer for generalization 
    model.add(layers.Dense(100, activation='relu'))
    model.add(layers.Dropout(0.5,training=False))
    
    #Last Dense layer for Reducing the dimensional for getting the needed classes. 
    #model.add(layers.Dense(1, activation='sigmoid'))
    model.add(layers.Dense(2, activation='softmax'))
    
    model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))
    opt = tensorflow.keras.optimizers.Adam(learning_rate=0.001)
    model.compile(loss='categorical_crossentropy', 
                   optimizer=opt, metrics=['binary_accuracy', 'categorical_accuracy']) 


        
def benchmark(dataset, num_epochs=2):
    start_time = time.perf_counter()
    for epoch_num in range(num_epochs):
        for sample in dataset:
            # Performing a training step
            time.sleep(0.01)
    print("Execution time:", time.perf_counter() - start_time)



## Visualizing dataset batches 
def plot_batch_sizes(ds,name):
  batch_sizes = [(numpy.asarray(batch)).shape[0] for batch in ds]
  plt.bar(range(len(batch_sizes)), batch_sizes)
  plt.xlabel('Batch number')
  plt.ylabel('Batch size')
  plt.title(name)
  
#plot_batch_sizes(X_Test_data,"Test Dataset")  

# Pre-processing 
def norm(x,_min,_max):
   return (x - _min)/(_max - _min)

def normalize_samples(feature,label):
   X = norm(feature,0,255)
   Y = label
   return X,Y
##Using the dataset batch function, shuffle and divide the dataset into batches
X_Test_data_norm = X_Test_data.map(normalize_samples)

# Batch split 
X_Test_data_norm = X_Test_data_norm.shuffle(5000).batch(10)


"""######################## Evaluation ######################"""
#Getting predicted values on test dataset from model
result = model.predict(X_Test_data_norm,verbose=1)
y_pred = [numpy.argmax(i) for i in result]
y_pred = numpy.asarray(y_pred)
print(y_pred)
#calculate the true label for test dataset
y_true = []
for i in X_Test_data_norm.unbatch():
    y_true.append(numpy.argmax(i[1].numpy()))
y_true = numpy.asarray(y_true)
print(y_true)
#find the confusion matrix
acc = accuracy_score(y_pred,y_true)

cm = confusion_matrix(y_true, y_pred)
#precision =  cm[0,0]/(cm[0,0] + cm[0,1])
#recall = cm[0,0]/(cm[0,0] + cm[1,0])
temp = precision_recall_fscore_support(y_true, y_pred, average='binary')
precision = temp[0]
recall = temp[1]
F1score = temp[2]
#F1score = f1_score(y_true, y_pred, average='weighted')
#F1score = 2 * ((precision * recall) / (precision + recall))

#Loss
# epochs = numpy.arange(1, EPOCHS + 1)
# plt.plot(epochs, history.history['loss'],label = "Train_loss")
# plt.plot(epochs, history.history['val_loss'], label = "Valid_loss" )
# plt.xlabel('epochs')
# plt.ylabel('loss')
# plt.legend()
# plt.show()

# #Accuracy
# plt.plot(epochs, history.history['accuracy'],label = "Train_acc")
# plt.plot(epochs, history.history['val_accuracy'],label = "valid_Acc")
# plt.legend()
# plt.xlabel('epochs')
# plt.ylabel('accuracy')
# plt.show()

#test_loss, binary_accuracy,categorical_accuracy  = model.evaluate(X_Test_data_norm,verbose = 1)
print(" ")
#print("On Test Set: (loss,binary_acc,categorical_acc): %.5f %.5f %.5f" % (test_loss,binary_accuracy,categorical_accuracy))
#F1score = f1_score(y_true,y_pred)
print("sklearn accuracy : {}".format(acc))
print("Precision: ", precision)
print("Recall: ", recall)
print("F1_Score: ", F1score)
print("confusion matrix:")
print("")
print(cm)

