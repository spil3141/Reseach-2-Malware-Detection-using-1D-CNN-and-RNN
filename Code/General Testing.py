
import numpy
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Activation, Reshape, LSTM, Bidirectional
from tensorflow.keras.layers import Embedding, Reshape, TimeDistributed
from tensorflow.keras.layers import Flatten,Conv1D, GlobalMaxPooling1D,MaxPooling1D,GlobalAveragePooling1D
import tensorflow as tf
from tensorflow.keras import layers
from sklearn.metrics import confusion_matrix
import tensorflow
import pandas as pd
from datetime import datetime
from sklearn.preprocessing import StandardScaler
import h5py
import numpy
import os
from sklearn.metrics import f1_score
from sklearn.metrics import precision_recall_fscore_support
import time

"""################ loading from file #################"""
# Loading Dataset from Hdf5 file format

filename = "D:\Dataset 3.1)(2000 samples).hdf5"

def train_batch_generator():
    with h5py.File(filename, "r") as f:
        for X,Y in zip(f["/Dataset 3.1/Train/Feature"],f["/Dataset 3.1/Train/Label"]):
            #make sure sample and target type is a numpy array
            Y = tf.one_hot(Y, depth=2)
            yield (X,Y)
def valid_batch_generator():
    with h5py.File(filename, "r") as f:
        for X,Y in zip(f["/Dataset 3.1/Valid/Feature"],f["/Dataset 3.1/Valid/Label"]):
            #make sure sample and target type is a numpy array
            Y = tf.one_hot(Y, depth=2)
            yield (X,Y)
def test_batch_generator():
    with h5py.File(filename, "r") as f:
        for X,Y in zip(f["/Dataset 3.1/Test/Feature"],f["/Dataset 3.1/Test/Label"]):
            #make sure sample and target type is a numpy array
            Y = tf.one_hot(Y, depth=2)
            yield (X,Y)

X_Train_data = tf.data.Dataset.from_generator(train_batch_generator,
                                          output_types=('float32', 'int32'),
                                          output_shapes=(tf.TensorShape((None,)), tf.TensorShape((None,)))
                                         )
X_Valid_data = tf.data.Dataset.from_generator(valid_batch_generator,
                                          output_types=('float32', 'int32'),
                                          output_shapes=(tf.TensorShape((None,)), tf.TensorShape((None,)))
                                         )
X_Test_data = tf.data.Dataset.from_generator(test_batch_generator,
                                          output_types=('float32', 'int32'),
                                          output_shapes=(tf.TensorShape((None,)), tf.TensorShape((None,)))
                                         )


checkpoint_dir = './training_checkpoints'
with h5py.File(filename, "r") as f:
   maxlen_pad = f["Dataset 3.1/Valid/Feature"][0].shape[0]
n_timesteps, n_features, n_input = int(maxlen_pad / 4), 4, maxlen_pad
strategy = tf.distribute.MirroredStrategy()
print('Number of devices: {}'.format(strategy.num_replicas_in_sync))
with strategy.scope():
    n_timesteps, n_features, n_input = int(maxlen_pad / 4), 4, maxlen_pad

    inputs = tf.keras.layers.Input(shape=(n_input,))

    #Reshape the input dataset to fits the 1D CNN input requirements.
    x = tf.keras.layers.Reshape((n_timesteps, n_features))(inputs)
    #1D CNN for 1D feature extractions
    x = tf.keras.layers.Conv1D(64,64,strides=16, activation='relu')(x)
    x = tf.keras.layers.MaxPooling1D(pool_size = 2)(x)
    x = tf.keras.layers.Dropout(0.3)(x,training=False)
    #Bi-D LSTM layer for sequential data learning 
    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16))(x)
    #Last Dense layer for Reducing the dimensional for getting the needed classes. 
    x = tf.keras.layers.Dense(100, activation='relu')(x)
    x = tf.keras.layers.Dropout(0.5)(x,training=False)(x)
    outputs = tf.keras.layers.Dense(2, activation='softmax')(x)
    model = tf.keras.Model(inputs=inputs,outputs=outputs)
    
    model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))
    opt = tensorflow.keras.optimizers.Adam(learning_rate=0.001)
    model.compile(loss='categorical_crossentropy', 
                   optimizer=opt, metrics=['binary_accuracy', 'categorical_accuracy']) 






# Pre-processing 
def norm(x,_min,_max):
   return (x - _min)/(_max - _min)

def normalize_samples(feature,label):
   X = norm(feature,0,255)
   Y = label
   return X,Y
##Using the dataset batch function, shuffle and divide the dataset into batches
X_Train_data_norm = X_Train_data.map(normalize_samples)
X_Valid_data_norm = X_Valid_data.map(normalize_samples)
X_Test_data_norm = X_Test_data.map(normalize_samples)

# Batch split 
X_Train_data_norm = X_Train_data_norm.shuffle(5000).batch(10)
X_Valid_data_norm = X_Valid_data_norm.shuffle(5000).batch(10)
X_Test_data_norm = X_Test_data_norm.shuffle(5000).batch(10)


