
"""#################################################################################"""
# from androguard.core.bytecodes.apk import APK
# from androguard.core.bytecodes.dvm import DalvikVMFormat
# import os
# import sys
# import math
# import numpy
# from PIL import Image
# from androguard.misc import AnalyzeAPK

# from keras.preprocessing import sequence
# from keras.models import Sequential
# from keras.layers import Dense, Dropout, Activation, Reshape
# from keras.layers import Embedding
# from keras.layers import Conv1D, GlobalMaxPooling1D
# from keras.datasets import imdb
# from sklearn.model_selection import train_test_split


# #import timeit
# #import multiprocessing
# #import PIL
# """############################# Paths to Important Directories ###########################################"""

# path_to_mal_apks_dir = "G:/Complete_Dataset/Raw Apk Files/Sorted/FInal Full Evaluation/Malware Apks"
# #path_to_mal_img_dir = "G:/Complete_Dataset/Raw Apk Files/Sorted/Malware Imgs"

# path_to_benign_apks_dir = "G:/Complete_Dataset/Raw Apk Files/Sorted/FInal Full Evaluation/Benign Apks"
# #path_to_benign_img_dir = "G:/Complete_Dataset/Raw Apk Files/Sorted/Benign Imgs"

# path_to_mal_img_dir = "G:/Complete_Dataset/Raw Apk Files/Sorted/FInal Full Evaluation/Dex/Malware Imgs"
# path_to_benign_img_dir = "G:/Complete_Dataset/Raw Apk Files/Sorted/FInal Full Evaluation/Dex/Benign Imgs"


# """############################# Conversion Functions ###########################################"""
# def From_apk_to_Data_Section(path_of_apk):
#     apk = path_of_apk
#     apk_obj = APK(apk)
#     dalvik_obj = DalvikVMFormat(apk_obj)
#     return dalvik_obj.get_buff()[dalvik_obj.header.data_off:] # using the offset from the start of file to start of the data section we get can get the data section


# """################ storing data in variables #################"""
# mal_data_sections = []
# benign_data_sections = []

# #Malware APKs
# for path in os.listdir(path_to_mal_apks_dir)[:2]:
#     mal_data_sections.append(From_apk_to_Data_Section(os.path.join(path_to_mal_apks_dir,path)))


# #Benign APKs
# for path in os.listdir(path_to_benign_apks_dir)[:2]:
#     benign_data_sections.append(From_apk_to_Data_Section(os.path.join(path_to_benign_apks_dir,path)))

# mal_data_sections = numpy.asarray(mal_data_sections)
# benign_data_sections =  numpy.asarray(benign_data_sections)


# X_combined = numpy.concatenate((mal_data_sections, benign_data_sections))
# Y_combined = [] 
# for i in range(len(X_combined)):
#     if( i < len(X_combined)/2):
#         Y_combined.append(1)
#     else:
#         Y_combined.append(0)

# X_train, X_test, y_train, y_test = train_test_split(X_combined, Y_combined, test_size=0.33, random_state=42,shuffle = True)



# """################ Saving to file #################"""

# print("Saving")
# numpy.savez_compressed( os.path.join("", "(" + str(len(X_combined)) + "x100)(1D Data Section Extraction).npz"),
#                     X_train= X_train,
#                     y_train= y_train,
#                     X_test = X_test,
#                     y_test = y_test)
# print("Finished")

# y_train = numpy.asarray(y_train)
# y_test = numpy.asarray(y_test)

# del(X_combined)
# del(Y_combined)
# del(mal_data_sections)
# del(benign_data_sections)

# """################ loading to file #################"""

# print("Loading Data")
# data = numpy.load("(100x100)(1D Data Section Extraction).npz")
# X_train= data['X_train']
# y_train= data['y_train']
# X_test = data['X_test']
# y_test = data['y_test']
# print("Done")
        
        
# #Padding 

# maxlen_pad = 30000
# print('Pad sequences (samples x time)')
# x_train_pad = sequence.pad_sequences(X_train, maxlen= maxlen_pad)
# x_test_pad = sequence.pad_sequences(X_test, maxlen=maxlen_pad)
# print('x_train shape:', x_train_pad.shape)
# print('x_test shape:', x_test_pad.shape)


"""#################################################################################"""
# """############################  model #####################################"""



# print('Build model...')
# model = Sequential()

# # we start off with an efficient embedding layer which maps
# # our vocab indices into embedding_dims dimensions
# model.add(Embedding(255,
#                     128, # random value to represent each byte
#                     input_length=maxlen_pad))
# model.add(Dropout(0.2))

# # model.add(Reshape((maxlen_pad, 1), input_shape=(maxlen_pad,)))
# # model.add(Dropout(0.2))

# # we add a Convolution1D, which will learn filters
# # word group filters of size filter_length:
# model.add(Conv1D(32,
#                  9,
#                  padding='valid',
#                  activation='relu',
#                  strides=1))
# # we use max pooling:
# model.add(GlobalMaxPooling1D())

# # We add a vanilla hidden layer:
# model.add(Dense(255))
# model.add(Dropout(0.2))
# model.add(Activation('relu'))

# # We project onto a single unit output layer, and squash it with a sigmoid:
# model.add(Dense(1))
# model.add(Activation('sigmoid'))

# model.summary()

# model.compile(loss='binary_crossentropy',
#               optimizer='adam',
#               metrics=['accuracy'])
# model.fit(x_train_pad,
#           y_train,
#           batch_size=1,
#           epochs=10,
#           validation_data=(x_test_pad, y_test))
"""#################################################################################"""

# import keras
# from tensorflow.keras.models import Sequential
# from tensorflow.keras.layers import Dense, Activation, LSTM, GRU, SimpleRNN, Conv1D, TimeDistributed, MaxPooling1D, Flatten, Dropout
# from tensorflow.keras import optimizers
# from tensorflow.keras.callbacks import EarlyStopping

# def init_model():
#     model = Sequential()
#     model.add(TimeDistributed(Conv1D(filters=8, kernel_size=2, activation='relu', padding='same'), batch_input_shape=(None, None, 4, 18)))
#     model.add(TimeDistributed(MaxPooling1D(pool_size=2)))
#     model.add(TimeDistributed(Flatten()))
#     model.add(LSTM(16))
#     model.add(Dense(9, activation='softmax'))
    
#     adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)

#     model.compile(optimizer=adam,
#                   loss='categorical_crossentropy',
#                   metrics=['accuracy'])
#     return model


# model = init_model()
# """#################################################################################"""

# ## A little script to balance my new dataset based on the names of corrupted files in a text file. 

# import os 

# path_to_mal_apks_target = "G:/Complete_Dataset/Raw Apk Files/Sorted/FInal Full Evaluation/Malware Apks"
# path_to_mal_apk_repository = "G:/Complete_Dataset/Raw Apk Files/Sorted/Malware Apks"

# files_in_target_dir = [i for i in os.listdir(path_to_mal_apks_target)]
# list_of_unadded_files = []

# for file in os.listdir(path_to_mal_apk_repository):
#     if file not in files_in_target_dir:
#         list_of_unadded_files.append(file)


# # Testing APK 
# from androguard.core.bytecodes.apk import APK
# from androguard.core.bytecodes.dvm import DalvikVMFormat
# import os
# def From_apk_to_Data_Section(path_of_apk):
#     apk = path_of_apk
#     apk_obj = APK(apk)
#     dalvik_obj = DalvikVMFormat(apk_obj)
#     return dalvik_obj.get_buff()[dalvik_obj.header.data_off:] # using the offset from the start of file to start of the data section we get can get the data section

# From_apk_to_Data_Section(os.path.join(path_to_mal_apks_dir,path)

# from sklearn.model_selection import train_test_split
# import numpy
# import os

# dataset_dir = "Dataset 2.0 (completed)/(20000xunknow)(1D Data Section Extraction-padded).npz"


# print("Loading Data")
# data = numpy.load(dataset_dir)
# X_train = data['X_train']
# y_train = data['y_train']
# X = data["X_test"]
# Y = data["y_test"]
# print("Done")

# X_test,X_valid,y_test,y_valid = train_test_split(X,Y,
#                                                     test_size=.5,
#                                                     random_state=42,
#                                                     shuffle = True)

# print("Saving")
# numpy.savez_compressed(os.path.join("",
#                                     "1D Data Section Extraction-padded plus valid set.npz"),
#                     X_train= X_train,
#                     y_train= y_train,
#                     X_test = X_test,
#                     y_test = y_test,
#                     X_valid = X_valid,
#                     y_valid = y_valid)
# print("Finished")

# import numpy
# import matplotlib.pyplot as plt
# from tensorflow.keras.models import Sequential
# from tensorflow.keras.layers import Dense, Dropout, Activation, Reshape, LSTM, Bidirectional
# from tensorflow.keras.layers import Embedding, Reshape, TimeDistributed
# from tensorflow.keras.layers import Flatten,Conv1D, GlobalMaxPooling1D,MaxPooling1D,GlobalAveragePooling1D
# import tensorflow as tf
# from tensorflow.keras import layers


# maxlen_pad = None
# EPOCHS = None
# Batch_size = None
# """################ loading from file #################"""

# # print("Loading Data")
# # data = numpy.load("Dataset 2.0 (completed)/(20000xunknow)(1D Data Section Extraction-padded).npz")
# # X_train= data['X_train']
# # y_train= data['y_train']
# # X_test = data["X_test"]
# # y_test = data["y_test"]

# data = numpy.load("(1500xunknow)(1D Data Section Extraction-padded plus valid set).npz")
# X_train= data['X_train']
# y_train= data['y_train']
# X_test = data["X_test"]
# y_test = data["y_test"]
# X_valid = data['X_valid']
# y_valid = data['y_valid']
# print("valid Dataset Size : ", X_valid.shape)
# print("Done")

# print("Training Dataset Size : ", X_train.shape)      
# print("Test Dataset Size : ", X_test.shape)

# from tensorflow.keras.models import Sequential
# from tensorflow.keras import layers
# from tensorflow.keras.layers import Reshape
# from tensorflow.keras.optimizers import RMSprop

# model = Sequential()
# n_timesteps, n_features, n_input = int(30000 / 4), 4, 30000
# model.add(Reshape((n_timesteps, n_features), input_shape=(30000,)))
# model.add(layers.Conv1D(32, 5, activation='relu',
#                         input_shape=(None, 500)))
# model.add(layers.MaxPooling1D(3))
# model.add(layers.Conv1D(32, 5, activation='relu'))
# model.add(Bidirectional(LSTM(32)))
# model.add(layers.Dense(1, activation='sigmoid'))
# model.summary()

# model.compile(loss='binary_crossentropy', 
#               optimizer='adam', metrics=['acc'])  
# history = model.fit(X_train, y_train, 
#                     batch_size= 64, epochs=5,
#                     validation_data = (X_valid,y_valid))

"""#### Importting library ###"""
from androguard.core.bytecodes.apk import APK
from androguard.core.bytecodes.dvm import DalvikVMFormat
import os
import numpy
from keras.preprocessing import sequence
from sklearn.model_selection import train_test_split
 
"""############################# Paths to Important Directories ###########################################"""

path_to_mal_apks_dir = "G:/Complete_Dataset/Raw Apk Files/Sorted/FInal Full Evaluation/Malware Apks"
path_to_benign_apks_dir = "G:/Complete_Dataset/Raw Apk Files/Sorted/FInal Full Evaluation/Benign Apks"



"""############################# Conversion Functions ###########################################"""
def From_apk_to_Data_Section(path_of_apk):
    apk = path_of_apk
    apk_obj = APK(apk)
    dalvik_obj = DalvikVMFormat(apk_obj)
    return dalvik_obj.get_buff()[dalvik_obj.header.data_off:] # using the offset from the start of file to 
                                                              #start of the data section we get can get the 
                                                              #data section


"""################ storing data in variables #################"""
size = []

###Malware APKs
cnt = 0 #This keep track of the amoung of error during conversion
file_w8_error = []
print("--- Start malware APKs conversion ---")
for path in os.listdir(path_to_mal_apks_dir)[:10]:
    try:
        size.append(len(From_apk_to_Data_Section(os.path.join(path_to_mal_apks_dir,path))))
    except:
        pass






