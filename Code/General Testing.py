from creme import datasets

X_y = datasets.Bikes()
x, y = next(iter(X_y))


from creme import compose

model = compose.Select('clouds', 'humidity', 'pressure', 'temperature', 'wind')


from creme import feature_extraction
from creme import stats

def add_hour(x):
    x['hour'] = x['moment'].hour
    return x

model += (
    add_hour |
    feature_extraction.TargetAgg(by=['station', 'hour'], how=stats.Mean())
)

model += feature_extraction.TargetAgg(by='station', how=stats.EWMean(0.5))


from creme import linear_model
from creme import preprocessing

model |= preprocessing.StandardScaler()
model |= linear_model.LinearRegression()

model.predict_one(x)



for x, y in X_y.take(10):
    print(model.predict_one(x))
    model.fit_one(x, y)


"""#################################################################################"""
# from androguard.core.bytecodes.apk import APK
# from androguard.core.bytecodes.dvm import DalvikVMFormat
# import os
# import sys
# import math
# import numpy
# from PIL import Image
# from androguard.misc import AnalyzeAPK

# from keras.preprocessing import sequence
# from keras.models import Sequential
# from keras.layers import Dense, Dropout, Activation, Reshape
# from keras.layers import Embedding
# from keras.layers import Conv1D, GlobalMaxPooling1D
# from keras.datasets import imdb
# from sklearn.model_selection import train_test_split


# #import timeit
# #import multiprocessing
# #import PIL
# """############################# Paths to Important Directories ###########################################"""

# path_to_mal_apks_dir = "G:/Complete_Dataset/Raw Apk Files/Sorted/FInal Full Evaluation/Malware Apks"
# #path_to_mal_img_dir = "G:/Complete_Dataset/Raw Apk Files/Sorted/Malware Imgs"

# path_to_benign_apks_dir = "G:/Complete_Dataset/Raw Apk Files/Sorted/FInal Full Evaluation/Benign Apks"
# #path_to_benign_img_dir = "G:/Complete_Dataset/Raw Apk Files/Sorted/Benign Imgs"

# path_to_mal_img_dir = "G:/Complete_Dataset/Raw Apk Files/Sorted/FInal Full Evaluation/Dex/Malware Imgs"
# path_to_benign_img_dir = "G:/Complete_Dataset/Raw Apk Files/Sorted/FInal Full Evaluation/Dex/Benign Imgs"


# """############################# Conversion Functions ###########################################"""
# def From_apk_to_Data_Section(path_of_apk):
#     apk = path_of_apk
#     apk_obj = APK(apk)
#     dalvik_obj = DalvikVMFormat(apk_obj)
#     return dalvik_obj.get_buff()[dalvik_obj.header.data_off:] # using the offset from the start of file to start of the data section we get can get the data section


# """################ storing data in variables #################"""
# mal_data_sections = []
# benign_data_sections = []

# #Malware APKs
# for path in os.listdir(path_to_mal_apks_dir)[:2]:
#     mal_data_sections.append(From_apk_to_Data_Section(os.path.join(path_to_mal_apks_dir,path)))


# #Benign APKs
# for path in os.listdir(path_to_benign_apks_dir)[:2]:
#     benign_data_sections.append(From_apk_to_Data_Section(os.path.join(path_to_benign_apks_dir,path)))

# mal_data_sections = numpy.asarray(mal_data_sections)
# benign_data_sections =  numpy.asarray(benign_data_sections)


# X_combined = numpy.concatenate((mal_data_sections, benign_data_sections))
# Y_combined = [] 
# for i in range(len(X_combined)):
#     if( i < len(X_combined)/2):
#         Y_combined.append(1)
#     else:
#         Y_combined.append(0)

# X_train, X_test, y_train, y_test = train_test_split(X_combined, Y_combined, test_size=0.33, random_state=42,shuffle = True)



# """################ Saving to file #################"""

# print("Saving")
# numpy.savez_compressed( os.path.join("", "(" + str(len(X_combined)) + "x100)(1D Data Section Extraction).npz"),
#                     X_train= X_train,
#                     y_train= y_train,
#                     X_test = X_test,
#                     y_test = y_test)
# print("Finished")

# y_train = numpy.asarray(y_train)
# y_test = numpy.asarray(y_test)

# del(X_combined)
# del(Y_combined)
# del(mal_data_sections)
# del(benign_data_sections)

# """################ loading to file #################"""

# print("Loading Data")
# data = numpy.load("(100x100)(1D Data Section Extraction).npz")
# X_train= data['X_train']
# y_train= data['y_train']
# X_test = data['X_test']
# y_test = data['y_test']
# print("Done")
        
        
# #Padding 

# maxlen_pad = 30000
# print('Pad sequences (samples x time)')
# x_train_pad = sequence.pad_sequences(X_train, maxlen= maxlen_pad)
# x_test_pad = sequence.pad_sequences(X_test, maxlen=maxlen_pad)
# print('x_train shape:', x_train_pad.shape)
# print('x_test shape:', x_test_pad.shape)


"""#################################################################################"""
# """############################  model #####################################"""



# print('Build model...')
# model = Sequential()

# # we start off with an efficient embedding layer which maps
# # our vocab indices into embedding_dims dimensions
# model.add(Embedding(255,
#                     128, # random value to represent each byte
#                     input_length=maxlen_pad))
# model.add(Dropout(0.2))

# # model.add(Reshape((maxlen_pad, 1), input_shape=(maxlen_pad,)))
# # model.add(Dropout(0.2))

# # we add a Convolution1D, which will learn filters
# # word group filters of size filter_length:
# model.add(Conv1D(32,
#                  9,
#                  padding='valid',
#                  activation='relu',
#                  strides=1))
# # we use max pooling:
# model.add(GlobalMaxPooling1D())

# # We add a vanilla hidden layer:
# model.add(Dense(255))
# model.add(Dropout(0.2))
# model.add(Activation('relu'))

# # We project onto a single unit output layer, and squash it with a sigmoid:
# model.add(Dense(1))
# model.add(Activation('sigmoid'))

# model.summary()

# model.compile(loss='binary_crossentropy',
#               optimizer='adam',
#               metrics=['accuracy'])
# model.fit(x_train_pad,
#           y_train,
#           batch_size=1,
#           epochs=10,
#           validation_data=(x_test_pad, y_test))
"""#################################################################################"""

# import keras
# from tensorflow.keras.models import Sequential
# from tensorflow.keras.layers import Dense, Activation, LSTM, GRU, SimpleRNN, Conv1D, TimeDistributed, MaxPooling1D, Flatten, Dropout
# from tensorflow.keras import optimizers
# from tensorflow.keras.callbacks import EarlyStopping

# def init_model():
#     model = Sequential()
#     model.add(TimeDistributed(Conv1D(filters=8, kernel_size=2, activation='relu', padding='same'), batch_input_shape=(None, None, 4, 18)))
#     model.add(TimeDistributed(MaxPooling1D(pool_size=2)))
#     model.add(TimeDistributed(Flatten()))
#     model.add(LSTM(16))
#     model.add(Dense(9, activation='softmax'))
    
#     adam = optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)

#     model.compile(optimizer=adam,
#                   loss='categorical_crossentropy',
#                   metrics=['accuracy'])
#     return model


# model = init_model()
# """#################################################################################"""

# ## A little script to balance my new dataset based on the names of corrupted files in a text file. 

# import os 

# path_to_mal_apks_target = "G:/Complete_Dataset/Raw Apk Files/Sorted/FInal Full Evaluation/Malware Apks"
# path_to_mal_apk_repository = "G:/Complete_Dataset/Raw Apk Files/Sorted/Malware Apks"

# files_in_target_dir = [i for i in os.listdir(path_to_mal_apks_target)]
# list_of_unadded_files = []

# for file in os.listdir(path_to_mal_apk_repository):
#     if file not in files_in_target_dir:
#         list_of_unadded_files.append(file)


# # Testing APK 
# from androguard.core.bytecodes.apk import APK
# from androguard.core.bytecodes.dvm import DalvikVMFormat
# import os
# def From_apk_to_Data_Section(path_of_apk):
#     apk = path_of_apk
#     apk_obj = APK(apk)
#     dalvik_obj = DalvikVMFormat(apk_obj)
#     return dalvik_obj.get_buff()[dalvik_obj.header.data_off:] # using the offset from the start of file to start of the data section we get can get the data section

# From_apk_to_Data_Section(os.path.join(path_to_mal_apks_dir,path)

# from sklearn.model_selection import train_test_split
# import numpy
# import os

# dataset_dir = "Dataset 2.0 (completed)/(20000xunknow)(1D Data Section Extraction-padded).npz"


# print("Loading Data")
# data = numpy.load(dataset_dir)
# X_train = data['X_train']
# y_train = data['y_train']
# X = data["X_test"]
# Y = data["y_test"]
# print("Done")

# X_test,X_valid,y_test,y_valid = train_test_split(X,Y,
#                                                     test_size=.5,
#                                                     random_state=42,
#                                                     shuffle = True)

# print("Saving")
# numpy.savez_compressed(os.path.join("",
#                                     "1D Data Section Extraction-padded plus valid set.npz"),
#                     X_train= X_train,
#                     y_train= y_train,
#                     X_test = X_test,
#                     y_test = y_test,
#                     X_valid = X_valid,
#                     y_valid = y_valid)
# print("Finished")

# import numpy
# import matplotlib.pyplot as plt
# from tensorflow.keras.models import Sequential
# from tensorflow.keras.layers import Dense, Dropout, Activation, Reshape, LSTM, Bidirectional
# from tensorflow.keras.layers import Embedding, Reshape, TimeDistributed
# from tensorflow.keras.layers import Flatten,Conv1D, GlobalMaxPooling1D,MaxPooling1D,GlobalAveragePooling1D
# import tensorflow as tf
# from tensorflow.keras import layers


# maxlen_pad = None
# EPOCHS = None
# Batch_size = None
# """################ loading from file #################"""

# # print("Loading Data")
# # data = numpy.load("Dataset 2.0 (completed)/(20000xunknow)(1D Data Section Extraction-padded).npz")
# # X_train= data['X_train']
# # y_train= data['y_train']
# # X_test = data["X_test"]
# # y_test = data["y_test"]

# data = numpy.load("(1500xunknow)(1D Data Section Extraction-padded plus valid set).npz")
# X_train= data['X_train']
# y_train= data['y_train']
# X_test = data["X_test"]
# y_test = data["y_test"]
# X_valid = data['X_valid']
# y_valid = data['y_valid']
# print("valid Dataset Size : ", X_valid.shape)
# print("Done")

# print("Training Dataset Size : ", X_train.shape)      
# print("Test Dataset Size : ", X_test.shape)

# from tensorflow.keras.models import Sequential
# from tensorflow.keras import layers
# from tensorflow.keras.layers import Reshape
# from tensorflow.keras.optimizers import RMSprop

# model = Sequential()
# n_timesteps, n_features, n_input = int(30000 / 4), 4, 30000
# model.add(Reshape((n_timesteps, n_features), input_shape=(30000,)))
# model.add(layers.Conv1D(32, 5, activation='relu',
#                         input_shape=(None, 500)))
# model.add(layers.MaxPooling1D(3))
# model.add(layers.Conv1D(32, 5, activation='relu'))
# model.add(Bidirectional(LSTM(32)))
# model.add(layers.Dense(1, activation='sigmoid'))
# model.summary()

# model.compile(loss='binary_crossentropy', 
#               optimizer='adam', metrics=['acc'])  
# history = model.fit(X_train, y_train, 
#                     batch_size= 64, epochs=5,
#                     validation_data = (X_valid,y_valid))

# """#### Importting library ###"""
# from androguard.core.bytecodes.apk import APK
# from androguard.core.bytecodes.dvm import DalvikVMFormat
# import os
# import numpy
# from keras.preprocessing import sequence
# from sklearn.model_selection import train_test_split
 
# """############################# Paths to Important Directories ###########################################"""

# path_to_mal_apks_dir = "G:/Complete_Dataset/Raw Apk Files/Sorted/FInal Full Evaluation/Malware Apks"
# path_to_benign_apks_dir = "G:/Complete_Dataset/Raw Apk Files/Sorted/FInal Full Evaluation/Benign Apks"



# """############################# Conversion Functions ###########################################"""
# def From_apk_to_Data_Section(path_of_apk):
#     apk = path_of_apk
#     apk_obj = APK(apk)
#     dalvik_obj = DalvikVMFormat(apk_obj)
#     return dalvik_obj.get_buff()[dalvik_obj.header.data_off:] # using the offset from the start of file to 
#                                                               #start of the data section we get can get the 
#                                                               #data section


# """################ storing data in variables #################"""
# size = []

# ###Malware APKs
# cnt = 0 #This keep track of the amoung of error during conversion
# file_w8_error = []
# print("--- Start malware APKs conversion ---")
# for path in os.listdir(path_to_mal_apks_dir)[:10]:
#     try:
#         size.append(len(From_apk_to_Data_Section(os.path.join(path_to_mal_apks_dir,path))))
#     except:
#         pass


"""#### Importting library ###"""

import numpy
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout, Activation, Reshape, LSTM, Bidirectional
from tensorflow.keras.layers import Embedding, Reshape, TimeDistributed
from tensorflow.keras.layers import Flatten,Conv1D, GlobalMaxPooling1D,MaxPooling1D,GlobalAveragePooling1D
import tensorflow as tf
from tensorflow.keras import layers
from sklearn.metrics import confusion_matrix
import tensorflow
import pandas as pd
from datetime import datetime
from sklearn.preprocessing import StandardScaler

maxlen_pad = 6111324
EPOCHS = None
Batch_size = None
"""################ loading from file #################"""

# print("Loading Data")
# data = numpy.load("Dataset 2.0 (completed)/(20000xunknow)(1D Data Section Extraction-padded).npz")
# X_train= data['X_train']
# y_train= data['y_train']
# X_test = data["X_test"]
# y_test = data["y_test"]

# data = numpy.load("(20000x110000)(1D Data Section Extraction-padded).npz")
# X_train= data['X_train']
# y_train= data['y_train']
# X_test = data["X_test"]
# y_test = data["y_test"]
# X_valid = data['X_valid']
# y_valid = data['y_valid']
# print("Done")
# print("valid Dataset Size : ", X_valid.shape)
# print("Training Dataset Size : ", X_train.shape)      
# print("Test Dataset Size : ", X_test.shape)

# Loading Dataset from Hdf5 file format

filename = "D:/Datasets/(Dataset 3.0).hdf5"

import h5py
#stdsc = StandardScaler()
#X_train_std = stdsc.fit_transform()
def X_batch_generator():
    with h5py.File(filename, "r") as f:
        for X,Y in zip(f["/Dataset 3.0/Feature"],f["/Dataset 3.0/Label"]):
            #make sure sample and target type is a numpy array
            Y = tf.one_hot(Y, depth=2)
            yield (X,Y)

X_data_gen = tf.data.Dataset.from_generator(X_batch_generator,
                                          output_types=('int32', 'int32'),
                                          output_shapes=(tf.TensorShape((None,)), tf.TensorShape((None,)))
                                         )


## Using dataset map function we need to scale the dataset 

##Using the dataset batch function, shuffle and divide the dataset into batches
X_data_gen = X_data_gen.shuffle(5000).batch(4)

# for count_batch in X_data.repeat().take(2):
#   print((count_batch[0]))

print("Done")          



#standardization
# stdsc = StandardScaler()
# X_train_std = stdsc.fit_transform(X_train)
# X_valid_std = stdsc.transform(X_valid)
# X_test_std = stdsc.transform(X_test)

# """### One-Hot Encoding of classes ###"""
# y_train_onehot = tf.keras.utils.to_categorical(y_train)
# y_valid_onehot = tf.keras.utils.to_categorical(y_valid)
# y_test_onehot = tf.keras.utils.to_categorical(y_test)

# for i,j in enumerate(X_train):
#     X_train[i] = numpy.asarray(j,dtype=numpy.int32)
#     # X_valid[i] = numpy.asarray(j,dtype=numpy.int32)
#     print(i,type(j))
# y_train_onehot = tf.keras.utils.to_categorical(y_train)
# y_valid_onehot = tf.keras.utils.to_categorical(y_valid)
# print(y_train_onehot.shape)
# print(y_valid_onehot.shape)

# """############################  model #####################################"""



# def create_model1():
#     print('Build model...')
#     model = Sequential()
#     n_timesteps, n_features, n_input = int(30000 / 4), 4, 30000 
#     model.add(Reshape((n_timesteps, n_features), input_shape=(n_input,)))
#     # A Convolution1D layer
#     model.add(Conv1D(128,#filters (number of features))
#                       32,#kernel size
#                       padding='valid',
#                       activation='relu',
#                       strides=1))
#     model.add(LSTM(32))
#     model.add(layers.Dense(1, activation='sigmoid'))
#     return model 

# def create_model2():
#     print('Build model...')
#     model = Sequential()
#     n_timesteps, n_features, n_input = int(30000 / 8), 8, 30000
#     model.add(Reshape((n_timesteps, n_features), input_shape=(n_input,)))
#     model.add(layers.Conv1D(32, 5, activation='relu',
#                             input_shape=(None, n_input)))
#     model.add(layers.MaxPooling1D(3))
#     model.add(Dropout(0.2))
#     model.add(layers.Conv1D(32, 5, activation='relu'))
#     model.add(Bidirectional(LSTM(32)))
#     model.add(layers.Dense(164, activation='relu'))
#     model.add(layers.Dropout(0.5))
#     # model.add(layers.Dense(1, activation='sigmoid'))
#     model.add(layers.Dense(2, activation='softmax'))
#     return model 
# def create_Testmodel():
#     print('Build model...')
#     model = Sequential()
#     n_timesteps, n_features, n_input = int(maxlen_pad / 4), 4, maxlen_pad
#     model.add(Reshape((n_timesteps, n_features), input_shape=(n_input,)))
#     #model.add(layers.Embedding(256,64,input_length=X_train.shape[1]))
#     model.add(layers.Conv1D(32,5, activation='relu'))
#     model.add(layers.BatchNormalization())
#     model.add(layers.MaxPooling1D(5))
#     model.add(layers.Dropout(0.3))
#     model.add(layers.Conv1D(32,5, activation='relu'))
#     model.add(layers.BatchNormalization())
#     model.add(layers.MaxPooling1D(5))
#     model.add(layers.Dropout(0.3))
#     model.add(Bidirectional(LSTM(16,  return_sequences=True)))
#     model.add(Bidirectional(LSTM(8)))
#     #model.add(layers.Flatten())
#     model.add(layers.Dense(100, activation='relu'))
#     model.add(layers.Dropout(0.3))
#     #model.add(layers.Dense(1, activation='sigmoid'))
#     model.add(layers.Dense(2, activation='softmax'))
#     return model 


# # model = create_model2()
# model = create_Testmodel()

# model.summary()

# # opt = tensorflow.keras.optimizers.Adam(learning_rate=0.001)
# # model.compile(loss='binary_crossentropy', 
# #               optimizer=opt, metrics=['acc']) 

# model.compile(optimizer= tf.keras.optimizers.SGD(learning_rate=0.01, #default = 0.01
#                                                          momentum=0.0,
#                                                          nesterov=False),
#                       loss="categorical_crossentropy",
#                       metrics = ["accuracy"])  


# EPOCHS = 10
# Batch_size = None

# # history = model.fit(X_train_std, y_train_onehot, 
# #                     batch_size= Batch_size, epochs=EPOCHS,
# #                     validation_data = (X_valid_std,y_valid_onehot)
# #                     )

# history = model.fit(X_data_gen,
#                     epochs=EPOCHS,
#                     verbose = 1
#                     )

# history = history.history
        
# """### Saving history ###"""
# hist_df = pd.DataFrame(history)
# # or save to csv:
# hist_csv_file = 'history.csv'
# with open(hist_csv_file, mode='w') as f:
#     hist_df.to_csv(f)
# # load history
# history = pd.read_csv("history.csv")

# """ Saving Model """
# # # Saving Model
# # model.save("Saved_Model_" + datetime.now().strftime("%Y%m%d-%H%M%S"))
# # #Loading
# # loaded_model = tf.keras.models.load_model('Saved_Model_20200515-183140')

# # opt = tensorflow.keras.optimizers.Adam(learning_rate=0.001)
# # loaded_model.compile(loss='binary_crossentropy', 
# #               optimizer=opt, metrics=['acc']) 

# """######################## Evaluation ######################"""
# #Loss
 

# epochs = numpy.arange(1, EPOCHS + 1)
# plt.plot(epochs, history['loss'],label = "Train_loss")
# plt.plot(epochs, history['val_loss'], label = "Valid_loss" )
# plt.xlabel('epochs')
# plt.ylabel('loss')
# plt.legend()
# plt.show()

# #Accuracy
# plt.plot(epochs, history['accuracy'],label = "Train_acc")
# plt.plot(epochs, history['val_accuracy'],label = "valid_Acc")
# plt.legend()
# plt.xlabel('epochs')
# plt.ylabel('accuracy')
# plt.show()

# #Evaluating
# # train_loss, train_acc = model.evaluate(X_train, y_train,verbose = 1)
# # valid_loss, valid_acc = loaded_model.evaluate(X_valid, y_valid,verbose = 1)
# test_loss, test_acc = model.evaluate(X_test_std, y_test_onehot,verbose = 0)
# # print("On Training Set: (loss,acc): %.5f %.5f" % (train_loss,train_acc))
# # print("On Validation Set: (loss,acc): %.5f %.5f " % (valid_loss, valid_acc))
# print("On Test Set: (loss,acc): %.5f %.5f" % (test_loss,test_acc))
# # # More evaluation
# X_test_temp = X_test.astype("float32")
# y_true = y_test
# result = model.predict(X_test_temp)
# y_pred = [numpy.argmax(i) for i in result]
# cm = confusion_matrix(y_true, y_pred)
# precision =  cm[0,0]/(cm[0,0] + cm[0,1])
# recall = cm[0,0]/(cm[0,0] + cm[1,0])
# F1score = 2 * ((precision * recall) / (precision + recall))
# #F1score = f1_score(y_true,y_pred)
# print("Precision: ", precision)
# print("Recall: ", recall)
# print("F1_Score: ", F1score)



