"""#### Importting library ###"""
from androguard.core.bytecodes.apk import APK
from androguard.core.bytecodes.dvm import DalvikVMFormat
import os
import numpy
from tensorflow.keras.preprocessing import sequence
from sklearn.model_selection import train_test_split
import h5py 
import random 
import platform
"""############################# Paths to Important Directories ###########################################"""

if platform.system() == "Windows":
    path_to_mal_apks_dir = "G:\Complete_Dataset\Raw Apk Files\Sorted\FInal Full Evaluation/Malware Apks"
    path_to_benign_apks_dir = "G:\Complete_Dataset\Raw Apk Files\Sorted\FInal Full Evaluation/Benign Apks"
    save_location = "D:\File1(20000 samples-padded)2.hdf5"
elif platform.system() == "Linux":
    path_to_mal_apks_dir = "/mnt/d/spil3141's Datasets/Malware Apks"
    path_to_benign_apks_dir = "/mnt/d/spil3141's Datasets/Benign Apks"
    save_location = "/mnt/e/File1(20000 samples-padded).hdf5"

"""############################# Conversion Functions ###########################################"""
def From_apk_to_Data_Section(path_of_apk):
    apk = path_of_apk
    apk_obj = APK(apk)
    dalvik_obj = DalvikVMFormat(apk_obj)
    return dalvik_obj.get_buff()[dalvik_obj.header.data_off:] # using the offset from the start of file to 
                                                              #start of the data section we get can get the 
                                                              #data section


"""################ storing data in variables #################"""
mal_data_sections = []
benign_data_sections = []
mal_count = 0 
benign_count =0

###Malware APKs
cnt = 0 #This keep track of the amoung of error during conversion
file_w8_error = []
print("--- Start malware APKs conversion ---")
for path in os.listdir(path_to_mal_apks_dir):
    print("On -> ",mal_count)
    try:
        mal_data_sections.append(numpy.asarray(From_apk_to_Data_Section(os.path.join(path_to_mal_apks_dir,path)),dtype="uint8"))
    except:
        cnt += 1
        file_w8_error.append(path)
        print("--- Error with %s" % (path))
        pass
    mal_count += 1
#Save list of invalid apks into a text file
with open("Error_Log(Files_dat_cant_be_converted_mal).txt","w+") as file:
    file.write("\n".join([ i for i in file_w8_error]))
print("--- Malware Conversion Ended ---")

####Benign APKs
print("--- Start benign APKs conversion ---")
cnt = 0 #This keep track of the amoung of error during conversion
file_w8_error = []
for path in os.listdir(path_to_benign_apks_dir):
    print("On -> ",benign_count)
    try:
        benign_data_sections.append(numpy.asarray(From_apk_to_Data_Section(os.path.join(path_to_benign_apks_dir,path)),dtype="uint8"))
    except:
        cnt += 1
        file_w8_error.append(path)
        print("--- Error with %s" % (path))
        pass
    benign_count += 1
#Save list of invalid apks into a text file
with open("Error_Log(Files_dat_cant_be_converted_benign).txt","w+") as file:
    file.write("\n".join([ i for i in file_w8_error]))
print("--- Benign Conversion Ended ---")


mal_data_sections = numpy.asarray(mal_data_sections)
benign_data_sections =  numpy.asarray(benign_data_sections)


"""### Combining and dividing the dataset into traning and test ###"""


X_combined = numpy.concatenate((mal_data_sections, benign_data_sections))
Y_combined = [] 
for i in range(len(X_combined)):
    if( i < len(X_combined)/2):
        Y_combined.append(1)
    else:
        Y_combined.append(0)
        
Y_combined = numpy.asarray(Y_combined)
print(X_combined.shape)
print(Y_combined.shape)
X_backup = X_combined
Y_backup = Y_combined


#shuffle dataset 
shuffled_combined = list(zip(X_combined,Y_combined))
random.shuffle(shuffled_combined)
X_combined, Y_combined = zip(*shuffled_combined)
X_combined, Y_combined = numpy.asarray(X_combined),numpy.asarray(Y_combined)




# X_combined max padding
def get_len(x):
    max_len = len(x[0])
    for i in x:
      if len(i) > max_len:
          max_len = len(i)
    return max_len

max_len = get_len(X_combined)

#check shapes before
for i in X_combined:
    print(i.shape)
#Post Padding
for i in range(len(X_combined)):
    X_combined[i] = (sequence.pad_sequences([X_combined[i]], maxlen= max_len, padding='post')).reshape(-1)
#check shapes after
for i in X_combined:
    print(i.shape)

""" Saving Dataset with different lengths as an HDF5 file format """

# # Creating h5py Group and Datasets
# filename = "D:\Dataset 1.0/File1(4000 samples-padded).hdf5"
# f = h5py.File(filename, 'w')
# f.create_group("Dataset 1.0")
# f.create_group("/Dataset 1.0/Train")
# f.create_group("/Dataset 1.0/Valid")
# f.create_group("/Dataset 1.0/Test")
# dt_x = h5py.special_dtype(vlen=numpy.dtype('uint8'))
# f.create_dataset("/Dataset 1.0/Train/Feature",data=X_combined[:15],dtype=dt_x) 
# f.create_dataset("/Dataset 1.0/Train/Label", data= Y_combined[:15],dtype="int32") 
# f.create_dataset("/Dataset 1.0/Valid/Feature", data=X_combined[15:17],dtype=dt_x) 
# f.create_dataset("/Dataset 1.0/Valid/Label", data=Y_combined[15:17],dtype="int32") 
# f.create_dataset("/Dataset 1.0/Test/Feature", data=X_combined[17:20],dtype=dt_x) 
# f.create_dataset("/Dataset 1.0/Test/Label", data=Y_combined[17:20],dtype="int32") 
# f.close()

""" Saving Dataset with padded lengths as an HDF5 file format """

# Creating h5py Group and Datasets
filename = save_location
f = h5py.File(filename, 'w')
f.create_group("Dataset 1.0")
f.create_group("/Dataset 1.0/Train")
f.create_group("/Dataset 1.0/Valid")
f.create_group("/Dataset 1.0/Test")
dt_x = h5py.special_dtype(vlen=numpy.dtype('float32'))
f.create_dataset("/Dataset 1.0/Train/Feature",data=X_combined[:18000],dtype=dt_x) 
f.create_dataset("/Dataset 1.0/Train/Label", data= Y_combined[:18000],dtype="int32") 
f.create_dataset("/Dataset 1.0/Valid/Feature", data=X_combined[18000:19000],dtype=dt_x) 
f.create_dataset("/Dataset 1.0/Valid/Label", data=Y_combined[18000:19000],dtype="int32") 
f.create_dataset("/Dataset 1.0/Test/Feature", data=X_combined[19000:20000],dtype=dt_x) 
f.create_dataset("/Dataset 1.0/Test/Label", data=Y_combined[19000:20000],dtype="int32") 
f.close()

#Testing Dataset 

# f = h5py.File(filename, 'r')
# for i in f["/Dataset 1.0/Train/Feature"]:
#     a = i
#     break
# f.close()
# with padding method 
""" Saving Dataset as an HDF5 file format """

## Creating h5py Group and Datasets

# def get_len(x):
#    max_len = len(x[0])
#    for i in x:
#       if len(i) > max_len:
#          max_len = len(i)
#    return max_len

# max_len = get_len(X_combined)

# filename = "D:\Dataset 1.0/File1(4000 samples-post-padded).hdf5"
# f = h5py.File(filename, 'w')
# f.create_group("Dataset 1.1")
# f.create_group("/Dataset 1.1/Train")
# f.create_group("/Dataset 1.1/Valid")
# f.create_group("/Dataset 1.1/Test")
# f.create_dataset("/Dataset 1.1/Train/Feature", (3000,max_len), dtype="int32") 
# f.create_dataset("/Dataset 1.1/Train/Label", (3000,), dtype="int32") 
# f.create_dataset("/Dataset 1.1/Valid/Feature", (500,max_len), dtype="int32") 
# f.create_dataset("/Dataset 1.1/Valid/Label", (500,), dtype="int32") 
# f.create_dataset("/Dataset 1.1/Test/Feature", (500,max_len), dtype="int32") 
# f.create_dataset("/Dataset 1.1/Test/Label", (500,), dtype="int32") 

# dataset_X_Train = f['/Dataset 1.1/Train/Feature']
# dataset_Y_Train = f["/Dataset 1.1/Train/Label"]
# dataset_X_Valid = f['/Dataset 1.1/Valid/Feature']
# dataset_Y_Valid = f["/Dataset 1.1/Valid/Label"]
# dataset_X_Test = f['/Dataset 1.1/Test/Feature']
# dataset_Y_Test = f["/Dataset 1.1/Test/Label"]

### Initializing Datasets Value


      
## Train
# count = 0
# for i in Y_combined[:3000]:
#     dataset_Y_Train[count] = i
#     count += 1
# count = 0
# for i in X_combined[:3000]:
#     if(len(i) != max_len):
#         # pad all samples to the lenght of the biggest apk file
#         i = (sequence.pad_sequences([i], maxlen= max_len, padding='post'))[0]
#     dataset_X_Train[count] = i
#     count += 1  
# ## Valid
# count = 0
# for i in Y_combined[3000:3500]:
#     dataset_Y_Valid[count] = i
#     count += 1
# count = 0
# for i in X_combined[3000:3500]:
#     if(len(i) != max_len):
#         # pad all samples to the lenght of the biggest apk file
#         i = (sequence.pad_sequences([i], maxlen= max_len, padding='post'))[0]
#     dataset_X_Valid[count] = i
#     count += 1  
# ## Test
# count = 0
# for i in Y_combined[3500:4000]:
#     dataset_Y_Test[count] = i
#     count += 1
# count = 0
# for i in X_combined[3500:4000]:
#     if(len(i) != max_len):
#         # pad all samples to the lenght of the biggest apk file
#         i = (sequence.pad_sequences([i], maxlen= max_len, padding='post'))[0]
#     dataset_X_Test[count] = i
#     count += 1  

# f.close()        



# X_train, X_test, y_train, y_test = train_test_split(X_combined, Y_combined,
#                                                     test_size=0.05,
#                                                     random_state=42,
#                                                     shuffle = True)


# y_train = numpy.asarray(y_train)
# y_test = numpy.asarray(y_test)

# """############## Padding ###############"""

# """###     Padding the samples to a size of 30,000 ###""" 

# maxlen_pad = 30000
# print('Pad sequences (samples x bytes)')
# x_train_pad = sequence.pad_sequences(X_train, maxlen= maxlen_pad)
# x_test_pad = sequence.pad_sequences(X_test, maxlen=maxlen_pad)
# print('x_train shape:', x_train_pad.shape)
# print('x_test shape:', x_test_pad.shape)



# """################ Saving to file as CSV format #################"""
# print("Saving")
# numpy.savez_compressed(os.path.join("",
#                                     "(" + str(len(X_combined)) + "xunknow)(1D Data Section Extraction-padded).npz"),
#                     X_train= x_train_pad,
#                     y_train= y_train,
#                     X_test = x_test_pad,
#                     y_test = y_test)
# print("Finished")

# # del(X_combined)
# # del(Y_combined)
# # del(mal_data_sections)
# # del(benign_data_sections)
# # del(i)
# # del(path)
# # del(path_to_mal_apks_dir)
# # del(path_to_benign_apks_dir)
# # del(path_to_mal_img_dir)
# # del(path_to_benign_img_dir)
