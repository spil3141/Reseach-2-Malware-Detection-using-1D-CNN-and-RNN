"""#### Importting library ###"""
from androguard.core.bytecodes.apk import APK
from androguard.core.bytecodes.dvm import DalvikVMFormat
import os
import numpy
from keras.preprocessing import sequence
from sklearn.model_selection import train_test_split
 
"""############################# Paths to Important Directories ###########################################"""

path_to_mal_apks_dir = "G:/Complete_Dataset/Raw Apk Files/Sorted/FInal Full Evaluation/Malware Apks"
path_to_benign_apks_dir = "G:/Complete_Dataset/Raw Apk Files/Sorted/FInal Full Evaluation/Benign Apks"



"""############################# Conversion Functions ###########################################"""
def From_apk_to_Data_Section(path_of_apk):
    apk = path_of_apk
    apk_obj = APK(apk)
    dalvik_obj = DalvikVMFormat(apk_obj)
    return dalvik_obj.get_buff()[dalvik_obj.header.data_off:] # using the offset from the start of file to 
                                                              #start of the data section we get can get the 
                                                              #data section


"""################ storing data in variables #################"""
mal_data_sections = []
benign_data_sections = []


###Malware APKs
cnt = 0 #This keep track of the amoung of error during conversion
file_w8_error = []
print("--- Start malware APKs conversion ---")
for path in os.listdir(path_to_mal_apks_dir):
    try:
        mal_data_sections.append(From_apk_to_Data_Section(os.path.join(path_to_mal_apks_dir,path)))
    except:
        cnt += 1
        file_w8_error.append(path)
        print("--- Error with %s" % (path))
        pass
#Save list of invalid apks into a text file
with open("Error_Log(Files_dat_cant_be_converted_mal).txt","w+") as file:
    file.write("\n".join([ i for i in file_w8_error]))
print("--- Malware Conversion Ended ---")

####Benign APKs
print("--- Start benign APKs conversion ---")
cnt = 0 #This keep track of the amoung of error during conversion
file_w8_error = []
for path in os.listdir(path_to_benign_apks_dir):
    try:
        benign_data_sections.append(From_apk_to_Data_Section(os.path.join(path_to_benign_apks_dir,path)))
    except:
        cnt += 1
        file_w8_error.append(path)
        print("--- Error with %s" % (path))
        pass
#Save list of invalid apks into a text file
with open("Error_Log(Files_dat_cant_be_converted_benign).txt","w+") as file:
    file.write("\n".join([ i for i in file_w8_error]))
print("--- Benign Conversion Ended ---")




"""### Combining and dividing the dataset into traning and test ###"""

mal_data_sections = numpy.asarray(mal_data_sections)
benign_data_sections =  numpy.asarray(benign_data_sections)

X_combined = numpy.concatenate((mal_data_sections, benign_data_sections))
Y_combined = [] 
for i in range(len(X_combined)):
    if( i < len(X_combined)/2):
        Y_combined.append(1)
    else:
        Y_combined.append(0)
        
Y_combined = numpy.asarray(Y_combined)
print(type(X_combined))
print(type(Y_combined))


""" Saving Dataset as an HDF5 file format """

## Creating h5py Group and Datasets
import h5py 

filename = "D:/Datasets/(Dataset 3.1).hdf5"
f = h5py.File(filename, 'w')
f.create_group("Dataset 3.1")
f.create_group("/Dataset 3.1/Train")
f.create_group("/Dataset 3.1/Valid")
f.create_group("/Dataset 3.1/Test")
f.create_dataset("/Dataset 3.1/Train/Feature", (X_combined.shape[0],len(X_combined.max())), dtype="int32") 
f.create_dataset("/Dataset 3.1/Train/Label", (Y_combined.shape[0],), dtype="int32") 
f.create_dataset("/Dataset 3.1/Valid/Feature", (X_combined.shape[0],len(X_combined.max())), dtype="int32") 
f.create_dataset("/Dataset 3.1/Valid/Label", (Y_combined.shape[0],), dtype="int32") 
f.create_dataset("/Dataset 3.1/Test/Feature", (X_combined.shape[0],len(X_combined.max())), dtype="int32") 
f.create_dataset("/Dataset 3.1/Test/Label", (Y_combined.shape[0],), dtype="int32") 

dataset_X_Train = f['/Dataset 3.1/Train/Feature']
dataset_Y_Train = f["/Dataset 3.1/Train/Label"]
dataset_X_Valid = f['/Dataset 3.1/Valid/Feature']
dataset_Y_Valid = f["/Dataset 3.1/Valid/Label"]
dataset_X_Test = f['/Dataset 3.1/Test/Feature']
dataset_Y_Test = f["/Dataset 3.1/Test/Label"]

### Initializing Datasets Value
## Train
count = 0
for i in Y_combined[:18000]:
    dataset_Y_Train[count] = i
    count += 1
count = 0
for i in X_combined[:18000]:
    if(len(i) != len(X_combined.max())):
        # pad all samples to the lenght of the biggest apk file
        i = (sequence.pad_sequences([i], maxlen= len(X_combined.max())))[0]
    dataset_X_Train[count] = i
    count += 1  
## Valid
count = 0
for i in Y_combined[18000:19000]:
    dataset_Y_Valid[count] = i
    count += 1
count = 0
for i in X_combined[18000:19000]:
    if(len(i) != len(X_combined.max())):
        # pad all samples to the lenght of the biggest apk file
        i = (sequence.pad_sequences([i], maxlen= len(X_combined.max())))[0]
    dataset_X_Valid[count] = i
    count += 1  
## Test
count = 0
for i in Y_combined[19000:20000]:
    dataset_Y_Test[count] = i
    count += 1
count = 0
for i in X_combined[19000:20000]:
    if(len(i) != len(X_combined.max())):
        # pad all samples to the lenght of the biggest apk file
        i = (sequence.pad_sequences([i], maxlen= len(X_combined.max())))[0]
    dataset_X_Test[count] = i
    count += 1  

f.close()   



# X_train, X_test, y_train, y_test = train_test_split(X_combined, Y_combined,
#                                                     test_size=0.05,
#                                                     random_state=42,
#                                                     shuffle = True)


# y_train = numpy.asarray(y_train)
# y_test = numpy.asarray(y_test)

# """############## Padding ###############"""

# """###     Padding the samples to a size of 30,000 ###""" 

# maxlen_pad = 30000
# print('Pad sequences (samples x bytes)')
# x_train_pad = sequence.pad_sequences(X_train, maxlen= maxlen_pad)
# x_test_pad = sequence.pad_sequences(X_test, maxlen=maxlen_pad)
# print('x_train shape:', x_train_pad.shape)
# print('x_test shape:', x_test_pad.shape)



# """################ Saving to file as CSV format #################"""
# print("Saving")
# numpy.savez_compressed(os.path.join("",
#                                     "(" + str(len(X_combined)) + "xunknow)(1D Data Section Extraction-padded).npz"),
#                     X_train= x_train_pad,
#                     y_train= y_train,
#                     X_test = x_test_pad,
#                     y_test = y_test)
# print("Finished")

# # del(X_combined)
# # del(Y_combined)
# # del(mal_data_sections)
# # del(benign_data_sections)
# # del(i)
# # del(path)
# # del(path_to_mal_apks_dir)
# # del(path_to_benign_apks_dir)
# # del(path_to_mal_img_dir)
# # del(path_to_benign_img_dir)
