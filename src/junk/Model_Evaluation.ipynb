{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#### Setup  ###\"\"\"\n",
    "\n",
    "import numpy\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import h5py\n",
    "import numpy\n",
    "import os\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import time\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"################ loading from file #################\"\"\"\n",
    "# Loading Dataset from Hdf5 file format\n",
    "file_location = \"/mnt/e/File1(20000 samples-padded).hdf5\"\n",
    "filename = file_location\n",
    "\n",
    "def test_batch_generator():\n",
    "    with h5py.File(filename, \"r\") as f:\n",
    "        for X,Y in zip(f[\"/Dataset 1.0/Test/Feature\"],f[\"/Dataset 1.0/Test/Label\"]):\n",
    "            #make sure sample and target type is a numpy array\n",
    "            Y = tf.one_hot(Y, depth=2)\n",
    "            yield (X,Y)\n",
    "\n",
    "\n",
    "X_Test_data = tf.data.Dataset.from_generator(test_batch_generator,\n",
    "                                          output_types=('float32', 'int32'),\n",
    "                                          output_shapes=(tf.TensorShape((None,)), tf.TensorShape((None,)))\n",
    "                                         )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in X_Test_data:\n",
    "    print(\"Input feature shape: \", i[0].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SEttin up model \n",
    "with h5py.File(filename, \"r\") as f:\n",
    "   maxlen_pad = f[\"Dataset 1.0/Valid/Feature\"][0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_timesteps, n_features, n_input = int(maxlen_pad / 4), 4, maxlen_pad\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "with strategy.scope():\n",
    "    n_timesteps, n_features= int(maxlen_pad / 4), 4\n",
    "\n",
    "    #Input\n",
    "    inputs = tf.keras.layers.Input(shape=(maxlen_pad,))\n",
    "    #Reshape the input dataset to fits the 1D CNN input requirements.\n",
    "    x = tf.keras.layers.Reshape((n_timesteps, n_features))(inputs)\n",
    "    #1D CNN for 1D feature extractions\n",
    "    x = tf.keras.layers.Conv1D(64,64,strides=8,kernel_regularizer=tf.keras.regularizers.l2(0.001), activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.3)(x,training=True)\n",
    "    x = tf.keras.layers.Conv1D(124,124,strides=1,kernel_regularizer=tf.keras.regularizers.l2(0.001), activation='relu')(x)\n",
    "    x = tf.keras.layers.Dropout(0.3)(x,training=True)\n",
    "    #Bi-D LSTM layer for sequential data learning \n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(100,return_sequences=True))(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x,training=True)\n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(100,return_sequences=False))(x)\n",
    "    x = tf.keras.layers.Dropout(0.5)(x,training=True)\n",
    "    outputs = tf.keras.layers.Dense(2)(x)\n",
    "    model = tf.keras.Model(inputs=inputs,outputs=outputs)\n",
    "#     #Input\n",
    "#     inputs = tf.keras.layers.Input(shape=(maxlen_pad,))\n",
    "#     #Reshape the input dataset to fits the 1D CNN input requirements.\n",
    "#     x = tf.keras.layers.Reshape((n_timesteps, n_features))(inputs)\n",
    "#     #1D CNN for 1D feature extractions\n",
    "#     x = tf.keras.layers.Conv1D(64,64,strides=16,kernel_regularizer=tf.keras.regularizers.l2(0.0005), activation='relu')(x)\n",
    "#     x = tf.keras.layers.Conv1D(64,64,strides=16,kernel_regularizer=tf.keras.regularizers.l2(0.0005), activation='relu')(x)\n",
    "#     x = tf.keras.layers.MaxPooling1D(pool_size = 2)(x)\n",
    "#     x = tf.keras.layers.Dropout(0.3)(x,training=True)\n",
    "#     x = tf.keras.layers.Conv1D(124,124,strides=1,kernel_regularizer=tf.keras.regularizers.l2(0.0005), activation='relu')(x)\n",
    "#     x = tf.keras.layers.MaxPooling1D(pool_size = 2)(x)\n",
    "#     x = tf.keras.layers.Dropout(0.3)(x,training=True)\n",
    "#     #Bi-D LSTM layer for sequential data learning \n",
    "#     x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(20))(x)\n",
    "#     x = tf.keras.layers.Dropout(0.3)(x,training=True)\n",
    "#     #Last Dense layer for Reducing the dimensional for getting the needed classes. \n",
    "#     x = tf.keras.layers.Dense(100,kernel_regularizer=tf.keras.regularizers.l2(0.0005), activation='relu')(x)\n",
    "#     x = tf.keras.layers.Dropout(0.5)(x,training=True)\n",
    "#     x = tf.keras.layers.Dense(100,kernel_regularizer=tf.keras.regularizers.l2(0.0005), activation='relu')(x)\n",
    "#     x = tf.keras.layers.Dropout(0.5)(x,training=True)\n",
    "#     outputs = tf.keras.layers.Dense(2)(x)\n",
    "#     model = tf.keras.Model(inputs=inputs,outputs=outputs)\n",
    "    \"\"\" Compiling mode\"\"\"\n",
    "    #opt = tensorflow.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    #model.compile(loss='binary_crossentropy', \n",
    "    #               optimizer=opt, metrics=['accuracy']) \n",
    "\n",
    "    #model.compile(optimizer='adam', loss='mae')\n",
    "\n",
    "    #model.compile(optimizer='adam',\n",
    "    #                  loss='categorical_crossentropy',\n",
    "    #                  metrics=['accuracy'])\n",
    "    # model.compile(optimizer= tf.keras.optimizers.SGD(learning_rate=0.01, #default = 0.01\n",
    "    #                                                  momentum=0.9,\n",
    "    #                                                  decay=1e-2,\n",
    "    #                                                  nesterov=False),\n",
    "    #                       loss=\"categorical_crossentropy\",\n",
    "    #                       metrics = [\"accuracy\"])   \n",
    "\n",
    "    #model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    #            optimizer=tf.keras.optimizers.Adam(),\n",
    "    #            metrics=['accuracy'])\n",
    "    model.load_weights(tf.train.latest_checkpoint('/mnt/e/training_checkpoints'))\n",
    "\n",
    "    opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "    model.compile(loss= tf.keras.losses.CategoricalCrossentropy(from_logits=True), \n",
    "                    optimizer=opt, metrics=['accuracy']) \n",
    "\n",
    "    model.summary()\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = '/mnt/e/training_checkpoints'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing \n",
    "def norm(x,_min,_max):\n",
    "   return (x - _min)/(_max - _min)\n",
    "\n",
    "def normalize_samples(feature,label):\n",
    "   X = norm(feature,0,255)\n",
    "   Y = label\n",
    "   return X,Y\n",
    "##Using the dataset batch function, shuffle and divide the dataset into batches\n",
    "X_Test_data_norm = X_Test_data.map(normalize_samples)\n",
    "\n",
    "# Batch split \n",
    "X_Test_data_norm = X_Test_data_norm.batch(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"######################## Evaluation ######################\"\"\"\n",
    "# Getting predicted values on test dataset from model\n",
    "print(\"prediction starts\")\n",
    "result = model.predict(X_Test_data_norm,verbose=1)\n",
    "y_pred = [numpy.argmax(i) for i in result]\n",
    "y_pred = numpy.asarray(y_pred)\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #calculate the true label for test dataset\n",
    "print(\"y_true extraction starts\")\n",
    "y_true = []\n",
    "for i in X_Test_data_norm.unbatch():\n",
    "    y_true.append(numpy.argmax(i[1].numpy()))\n",
    "y_true = numpy.asarray(y_true)\n",
    "print(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_cm(labels, predictions):\n",
    "#     cm = confusion_matrix(labels, predictions)\n",
    "#     plt.figure(figsize=(5,5))\n",
    "#     sns.heatmap(cm, annot=True, fmt=\"d\")\n",
    "#     plt.title('Confusion matrix @{:.2f}'.format(p))\n",
    "#     plt.ylabel('Actual label')\n",
    "#     plt.xlabel('Predicted label')\n",
    "\n",
    "# # find the confusion matrix\n",
    "print(\"result calculation starts\")\n",
    "acc = accuracy_score(y_pred,y_true)\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "# plot_cm(y_true,y_pred)\n",
    "temp = precision_recall_fscore_support(y_true, y_pred, average='weighted')\n",
    "precision = temp[0]\n",
    "recall = temp[1]\n",
    "F1score = temp[2]\n",
    "\n",
    "\n",
    "test_loss,test_accuracy  = model.evaluate(X_Test_data_norm,verbose = 1)\n",
    "print(\" \")\n",
    "print(\"On Test Set: (loss,acc): %.5f %.5f\" % (test_loss,test_accuracy))\n",
    "# #F1score = f1_score(y_true,y_pred)\n",
    "print(\"sklearn accuracy : {}\".format(acc))\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall: \", recall)\n",
    "print(\"F1_Score: \", F1score)\n",
    "print(\"confusion matrix:\")\n",
    "print(\"\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
