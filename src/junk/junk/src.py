import tensorflow as tf
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import os
from datetime import datetime
from sklearn.metrics import confusion_matrix
from sklearn.metrics import f1_score
import pandas as pd
from sklearn.preprocessing import StandardScaler
import joblib

class CNN_Models:
    #private attributes
    dataset_path = None
    data = None
    m_ImageSize = None
    X_train = None
    X_test = None
    y_test = None
    X_train_std = None
    X_valid_std = None
    X_test_std = None
    y_train_onehot = None
    y_valid_onehot = None
    y_test_onehot = None
    DatasetLoaded = False
    DataProcessingPhaseCompleted = False
    EPOCHS = None
    Batch_Size = None
    history = None
    
    #public attributes
    model = None
    Model_Is_Setup = False
    
    #private methods
    
    #public methods
    def __init__(self,path,imagesize):
        self.dataset_path = path
        self.m_ImageSize = imagesize
    
    def is_all_good(self):
        print(tf.__version__)
        print("GPU Available: ", tf.test.is_gpu_available())
        print("Dataset is loaded: ", self.DatasetLoaded)
        print("Data Processing Phase Completed: ",self.DataProcessingPhaseCompleted)
        print("Model is ready for training: ",self.Model_Is_Setup)
        
    def loadData(self):
        """### Dataset Import ####"""
        print("Loading Data")
        self.data = np.load(self.dataset_path)
        self.X_train= self.data['X_train']
        self.y_train= self.data['y_train']
        self.X_test = self.data['X_test']
        self.y_test = self.data['y_test']
        print("Done")
        
        #(X_train,y_train),(X_test,y_test) = tf.keras.datasets.mnist.load_data()
        #X_train = X_train.reshape((60000,-1))
        #X_test = X_test.reshape((10000,-1))
        #y_train = y_train
        #y_test = y_test
        
        
        """### Checking shape of imported dataset ###"""
        print(self.X_train.shape)
        print(self.y_train.shape)
        print(self.X_test.shape)
        print(self.y_test.shape)
        
        self.DatasetLoaded = True
    
    def PreprocessData(self):
        print("Data Preprocessing")
    
        # Generating Validation set from Training Set
        #a,b,c,d = train_test_split(X_train,y_train,test_size=.0526,stratify=y_train)
        self.X_train,self.X_valid,self.y_train,self.y_valid = train_test_split(self.X_train,self.y_train,test_size=.0526,stratify=self.y_train)
        #Reshaping samples to 1D for feature scaling
        self.X_train = self.X_train.reshape((-1,self.m_ImageSize[0] * self.m_ImageSize[1] ))
        self.X_valid = self.X_valid.reshape((-1,self.m_ImageSize[0] * self.m_ImageSize[1]))
        self.X_test = self.X_test.reshape((-1,self.m_ImageSize[0] * self.m_ImageSize[1]))
    
        print(self.X_train.shape)
        print(self.X_valid.shape)
        print(self.X_test.shape)
        #standardization
        self.stdsc = StandardScaler()
        self.X_train_std = self.stdsc.fit_transform(self.X_train)
        self.X_valid_std = self.stdsc.transform(self.X_valid)
        self.X_test_std = self.stdsc.transform(self.X_test)
        #Normalization
        #X_train_norm = X_train / 255.0
        #X_test_norm = X_test / 255.0
        #X_valid_norm = X_valid / 255.0
        
        #Reshaping samples to rank 4  tensors
#        self.X_train_std = self.X_train_std.reshape((-1,100,100,1))
#        self.X_valid_std = self.X_valid_std.reshape((-1,100,100,1))
#        self.X_test_std = self.X_test_std.reshape((-1,100,100,1))
        #X_train_std = X_train_std.reshape((-1,150,150,1))
        #X_valid_std = X_valid_std.reshape((-1,150,150,1))
        #X_test_std = X_test_std.reshape((-1,150,150,1))
#        self.X_train_std = self.X_train_std.reshape((-1,256,256,1))
#        self.X_valid_std = self.X_valid_std.reshape((-1,256,256,1))
#        self.X_test_std = self.X_test_std.reshape((-1,256,256,1))
        self.X_train_std = self.X_train_std.reshape((-1,self.m_ImageSize[0],self.m_ImageSize[1],1))
        self.X_valid_std = self.X_valid_std.reshape((-1,self.m_ImageSize[0],self.m_ImageSize[1],1))
        self.X_test_std = self.X_test_std.reshape((-1,self.m_ImageSize[0],self.m_ImageSize[1],1))
        print(self.X_train_std.shape)
        print(self.X_valid_std.shape)
        print(self.X_test_std.shape)
        print("Done")
        
        
        ##Saving the Feature Scaling Model for Future transformation of Unknown Samples
        joblib.dump(self.stdsc,"Scaler_Model.sav")
        print("Scaler Model Saved ")
        #loaded_scaler_model = joblib.load("Scaler_Model.sav")
        
        """### One-Hot Encoding of classes ###"""
        self.y_train_onehot = tf.keras.utils.to_categorical(self.y_train)
        self.y_valid_onehot = tf.keras.utils.to_categorical(self.y_valid)
        self.y_test_onehot = tf.keras.utils.to_categorical(self.y_test)
        print(self.y_train_onehot.shape)
        print(self.y_valid_onehot.shape)
        print(self.y_test_onehot.shape)
        
        self.DataProcessingPhaseCompleted = True

        """### CNN Models  ###"""
    def create_model_spil_cnn(self):
        model = tf.keras.models.Sequential()
        model.add(tf.keras.layers.Conv2D(32, (3, 3),padding = "same", activation='relu', input_shape= self.X_train_std.shape[1:]))
        model.add(tf.keras.layers.Conv2D(32, (3, 3),padding = "same", activation='relu'))
        model.add(tf.keras.layers.MaxPooling2D((2, 2)))
        model.add(tf.keras.layers.Dropout(0.45))
        model.add(tf.keras.layers.Conv2D(64, (3, 3),padding = "same", activation='relu'))
        model.add(tf.keras.layers.Conv2D(64, (3, 3),padding = "same", activation='relu'))
        model.add(tf.keras.layers.MaxPooling2D((2, 2)))
        model.add(tf.keras.layers.Dropout(0.45))
        model.add(tf.keras.layers.Flatten())
        model.add(tf.keras.layers.Dense(222, activation='relu'))
        model.add(tf.keras.layers.Dropout(0.45))
        model.add(tf.keras.layers.Dense(2, activation='softmax')) # softmax activation function goes with categorical_crossentropy
        # model.add(tf.keras.layers.Dense(1, activation = "sigmoid")) # if i use a sigmoid activation function i must pass a loss function of binary_crossentropy
        return model
    def create_model_vanilla_cnn(self,conv1num = 32, conv2num = 64,conv3num = 64):
        model = tf.keras.models.Sequential()
        model.add(tf.keras.layers.Conv2D(conv1num, (3, 3),padding = "same", activation='relu', input_shape=(self.X_train_std.shape[1:])))
        model.add(tf.keras.layers.MaxPooling2D((2, 2)))
        model.add(tf.keras.layers.Conv2D(conv2num, (3, 3),padding = "same", activation='relu'))
        model.add(tf.keras.layers.MaxPooling2D((2, 2)))
        model.add(tf.keras.layers.Conv2D(conv3num, (3, 3),padding = "same", activation='relu'))
        model.add(tf.keras.layers.Flatten())
        model.add(tf.keras.layers.Dense(64, activation='relu'))
        model.add(tf.keras.layers.Dropout(0.5))
        model.add(tf.keras.layers.Dense(2, activation='softmax')) # softmax activation function goes with categorical_crossentropy
    #    model.add(tf.keras.layers.Dense(1, activation = "sigmoid")) # if i use a sigmoid activation function i must pass a loss function of binary_crossentropy
        return model
    def create_inception_v3(self):
        input_tensor = tf.keras.layers.Input(shape=(150,150, 1))  # this assumes K.image_data_format() == 'channels_last'
        base_model = tf.keras.applications.inception_v3.InceptionV3(input_tensor = input_tensor,weights="imagenet", include_top=False)
        x = base_model.output
        x = tf.keras.layers.GlobalAveragePooling2D()(x)
        x = tf.keras.layers.Dense(2048, activation='relu')(x)
        predictions = tf.keras.layers.Dense(2, activation='softmax')(x)
        model = tf.keras.models.Model(inputs=base_model.input, outputs=predictions)
        return model
    def create_ResNet50(self):
        base_model = tf.keras.applications.resnet.ResNet50(include_top=False,
                                                     weights=None,
                                                     input_tensor= tf.keras.layers.Input(shape=(150,150, 1)),
                                                     input_shape= (150,150, 1),
                                                     pooling=None,
                                                     classes=2)
        x = base_model.output
        x = tf.keras.layers.GlobalAveragePooling2D()(x)
        x = tf.keras.layers.Dense(2048, activation='relu')(x)
        predictions = tf.keras.layers.Dense(2, activation='softmax')(x)
        model = tf.keras.models.Model(inputs=base_model.input, outputs=predictions)
        return model
    def create_ResNet101(self):
        base_model = tf.keras.applications.resnet.ResNet101(include_top=False,
                                                     weights=None,
                                                     input_tensor= tf.keras.layers.Input(shape=(150,150, 1)),
                                                     input_shape= (150,150, 1),
                                                     pooling=None,
                                                     classes=2)
        x = base_model.output
        x = tf.keras.layers.GlobalAveragePooling2D()(x)
        x = tf.keras.layers.Dense(2048, activation='relu')(x)
        predictions = tf.keras.layers.Dense(2, activation='softmax')(x)
        model = tf.keras.models.Model(inputs=base_model.input, outputs=predictions)
        return model
    def create_DenseNet121(self):
        base_model = tf.keras.applications.densenet.DenseNet121(include_top=False,
                                                           weights=None,
                                                           input_tensor= tf.keras.layers.Input(shape=(150,150, 1)),
                                                           input_shape=(150,150,1),
                                                           pooling=None,
                                                           classes=2)
        x = base_model.output
        x = tf.keras.layers.GlobalAveragePooling2D()(x)
        x = tf.keras.layers.Dense(2048, activation='relu')(x)
        predictions = tf.keras.layers.Dense(2, activation='softmax')(x)
        model = tf.keras.models.Model(inputs=base_model.input, outputs=predictions)
        return model
    def create_NASNet(self):
        base_model = tf.keras.applications.nasnet.NASNetMobile(input_shape= (150,150,1),
                                                              include_top=False,
                                                              weights= None,
                                                              input_tensor= tf.keras.layers.Input(shape=(150,150, 1)),
                                                              pooling=None,
                                                              classes=2)
        x = base_model.output
        x = tf.keras.layers.GlobalAveragePooling2D()(x)
        x = tf.keras.layers.Dense(1056, activation='relu')(x)
        predictions = tf.keras.layers.Dense(2, activation='softmax')(x)
        model = tf.keras.models.Model(inputs=base_model.input, outputs=predictions)
        return model
    def create_InceptionResNetV2(self) -> tf.keras.models.Model:
        base_model = tf.keras.applications.inception_resnet_v2.InceptionResNetV2(include_top=False,
                                                                                 input_shape= self.X_train_std.shape[1:],
                                                                                 weights=None,
                                                                                 input_tensor= tf.keras.layers.Input(shape= self.X_train_std.shape[1:]),
                                                                                 pooling=None,
    #                                                                             classes=2
                                                                                 )
        x = base_model.output
        x = tf.keras.layers.GlobalAveragePooling2D()(x)
        x = tf.keras.layers.Dense(64, activation='relu')(x)
        predictions = tf.keras.layers.Dense(2, activation='softmax')(x)
        model = tf.keras.models.Model(inputs=base_model.input, outputs=predictions)
        return model
    
    def compile_Model(self):
        #Compiling Model before traning
        
        self.model.compile(optimizer= tf.keras.optimizers.SGD(learning_rate=0.01,
                                                         momentum=0.0,
                                                         nesterov=False),
                      loss="categorical_crossentropy",
                      metrics = ["accuracy"])
        #model.compile(optimizer= tf.keras.optimizers.RMSprop(lr=0.001,
        #                                                     rho=0.9,
        #                                                     epsilon=None,
        #                                                     decay=0.0),
        #              loss='categorical_crossentropy',
        #              metrics = ["accuracy"])
        
        #model.compile(optimizer= tf.keras.optimizers.RMSprop(lr=0.045,
        #                                                     rho=0.9,
        #                                                     epsilon=0.9,
        #                                                     decay=1.0),
        #              loss='categorical_crossentropy',
        #              metrics = ["accuracy"])
        
        #model.compile(optimizer= tf.keras.optimizers.Adam(lr=0.001,
        #                                                  beta_1=0.9,
        #                                                  beta_2=0.999,
        #                                                  epsilon=None,
        #                                                  decay=0.0,
        #                                                  amsgrad=False),
        #               loss='categorical_crossentropy',
        ##                loss = "binary_crossentropy",
        #               metrics=['accuracy'])
        self.Model_Is_Setup = True

        
    def train_Model(self,epoch:int,batch_size: int):
        self.Batch_Size = batch_size
        self.EPOCHS = epoch
        
        """### Training ###"""
        #Saving Checkpoint At Every Epoch
        checkpoint_path = "training_1/cp-{epoch:04d}.ckpt"
        checkpoint_dir = os.path.dirname(checkpoint_path)
        #logdir = "logs/scalars/" + datetime.now().strftime("%Y%m%d-%H%M%S")
        #,tf.keras.callbacks.TensorBoard(log_dir=logdir)
        
        callback_list = [tf.keras.callbacks.ModelCheckpoint(
            checkpoint_path, verbose=0, save_weights_only=True,
            # Save weights, every 5-epochs.
            period=5)]
        
        #Fitting
        history = self.model.fit(self.X_train_std, self.y_train_onehot,
                             batch_size=self.Batch_Size, epochs=self.EPOCHS,
                             validation_data=(self.X_valid_std, self.y_valid_onehot),
                             callbacks= callback_list,
                             verbose=1)
        
        self.history = history.history
        
        """### Saving Completed Model ###"""
        # Saving Model
        #tf.keras.experimental.export_saved_model(model, "Saved_Model_" + datetime.now().strftime("%Y%m%d-%H%M%S"))
        #tf.saved_model.save(model,"Saved_Model_" + datetime.now().strftime("%Y%m%d-%H%M%S"))
        # model.save("SavedModel.h5")
        # Saving Training Record: convert the history.history dict to a pandas DataFrame:
        hist_df = pd.DataFrame(self.history)
        # or save to csv:
        hist_csv_file = 'history.csv'
        with open(hist_csv_file, mode='w') as f:
            hist_df.to_csv(f)
        # load history
        self.history = pd.read_csv("history.csv")
        """### Restoring Weights ###"""
        #new_model =c
        #new_model.compile(optimizer='rmsprop', loss='categorical_crossentropy',metrics = ["accuracy"])
        
        # # # #Loading
        #loaded_model = tf.saved_model.load("Saved_Model_20191023-135029")
        ##loaded_model.summary()
        #model = create_model_vanilla_cnn()
        ##new_model_2 = tf.keras.models.load_model("SavedModel.h5")
        #latest = tf.train.latest_checkpoint('training_1')
        ## Loads the weights
        #model.load_weights(latest)
        #model.summary()
        ## After Loading Model Compilation
        #model.compile(optimizer= tf.keras.optimizers.Adam(lr=0.001,
        ##                                                  beta_1=0.9,
        ##                                                  beta_2=0.999,
        #                                                  epsilon=None,
        #                                                  decay=0.0,
        #                                                  amsgrad=False),
        #               loss='categorical_crossentropy',
        ##                loss = "binary_crossentropy",
        #               metrics=['accuracy'])
        """### Recovering Saved training progress from Saved Checkpoints by loading weights ###"""
        # model = create_model_vanilla_cnn()
        #latest = tf.train.latest_checkpoint("training_1")
        # #Loading Saved Weigths
        # model.load_weights(latest)
        #Note: Compile After checkpoint loading
    
    def show_Result(self):
        """### Evaluation ###"""
        #Loss
        epochs = np.arange(1, self.EPOCHS + 1)
        plt.plot(epochs, self.history['loss'],label = "Train_loss")
        plt.plot(epochs, self.history['val_loss'], label = "Valid_loss" )
        plt.xlabel('epochs')
        plt.ylabel('loss')
        plt.legend()
        plt.show()
        
        #Accuracy
        plt.plot(epochs, self.history['acc'],label = "Train_acc")
        plt.plot(epochs, self.history['val_acc'],label = "valid_Acc")
        plt.legend()
        plt.xlabel('epochs')
        plt.ylabel('accuracy')
        plt.show()
        
        #Evaluating
        train_loss, train_acc = self.model.evaluate(self.X_train_std, self.y_train_onehot,verbose = 0)
        valid_loss, valid_acc = self.model.evaluate(self.X_valid_std, self.y_valid_onehot,verbose = 0)
        test_loss, test_acc = self.model.evaluate(self.X_test_std, self.y_test_onehot,verbose = 0)
        print("On Training Set: (loss,acc): ",train_loss,train_acc)
        print("On Validation Set: (loss,acc): ",valid_loss, valid_acc)
        print("On Test Set: (loss,acc): ",test_loss,test_acc)
        # More evaluation
        y_true = self.y_test
        y_pred = [np.argmax(i) for i in self.model.predict(self.X_test_std)]
        cm = confusion_matrix(y_true, y_pred)
        precision =  cm[0,0]/(cm[0,0] + cm[0,1])
        recall = cm[0,0]/(cm[0,0] + cm[1,0])
        F1score = 2 * ((precision * recall) / (precision + recall))
        #F1score = f1_score(y_true,y_pred)
        print("Precision: ", precision)
        print("Recall: ", recall)
        print("F1_Score: ", F1score)