"""#### Setup  ###"""

import numpy
import matplotlib.pyplot as plt
import tensorflow as tf
import pandas as pd
from datetime import datetime
from sklearn.preprocessing import StandardScaler
import h5py
import numpy
import os
from sklearn.metrics import f1_score
from sklearn.metrics import precision_recall_fscore_support
from sklearn.metrics import confusion_matrix
from sklearn.metrics import accuracy_score
import time

"""################ loading from file #################"""
# Loading Dataset from Hdf5 file format

filename = "/mnt/d/Dataset 2.0 (completed)/(Dataset 3.1)(8000 samples).hdf5"

def test_batch_generator():
    with h5py.File(filename, "r") as f:
        for X,Y in zip(f["/Dataset 3.1/Test/Feature"],f["/Dataset 3.1/Test/Label"]):
            #make sure sample and target type is a numpy array
            Y = tf.one_hot(Y, depth=2)
            yield (X,Y)


X_Test_data = tf.data.Dataset.from_generator(test_batch_generator,
                                          output_types=('float32', 'int32'),
                                          output_shapes=(tf.TensorShape((None,)), tf.TensorShape((None,)))
                                         )


y_true_data = tf.data.Dataset.from_generator(test_batch_generator,
                                          output_types=('float32', 'int32'),
                                          output_shapes=(tf.TensorShape((None,)), tf.TensorShape(()))
                                         )

#SEttin up model 
checkpoint_dir = './training_checkpoints'
with h5py.File(filename, "r") as f:
   maxlen_pad = f["Dataset 3.1/Valid/Feature"][0].shape[0]
n_timesteps, n_features, n_input = int(maxlen_pad / 4), 4, maxlen_pad
strategy = tf.distribute.MirroredStrategy()
print('Number of devices: {}'.format(strategy.num_replicas_in_sync))
with strategy.scope():
    n_timesteps, n_features= int(maxlen_pad / 4), 4

    #Input
    inputs = tf.keras.layers.Input(shape=(maxlen_pad,))
    #Reshape the input dataset to fits the 1D CNN input requirements.
    x = tf.keras.layers.Reshape((n_timesteps, n_features))(inputs)
    #1D CNN for 1D feature extractions
    x = tf.keras.layers.Conv1D(64,64,strides=16, activation='relu')(x)
    x = tf.keras.layers.MaxPooling1D(pool_size = 2)(x)
    x = tf.keras.layers.Dropout(0.3)(x,training=True)
    #Bi-D LSTM layer for sequential data learning 
    x = tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(16))(x)
    #Last Dense layer for Reducing the dimensional for getting the needed classes. 
    x = tf.keras.layers.Dense(100, activation='relu')(x)
    x = tf.keras.layers.Dropout(rate=0.5)(x, training=True) 
    outputs = tf.keras.layers.Dense(2, activation='softmax')(x)
    model = tf.keras.Model(inputs=inputs,outputs=outputs)
    
    model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))
    opt = tf.keras.optimizers.Adam(learning_rate=0.001)
    model.compile(loss='categorical_crossentropy', 
                    optimizer=opt, metrics=['accuracy']) 

model.summary()
        
def benchmark(dataset, num_epochs=2):
    start_time = time.perf_counter()
    for epoch_num in range(num_epochs):
        for sample in dataset:
            # Performing a training step
            time.sleep(0.01)
    print("Execution time:", time.perf_counter() - start_time)



## Visualizing dataset batches 
def plot_batch_sizes(ds,name):
  batch_sizes = [(numpy.asarray(batch)).shape[0] for batch in ds]
  plt.bar(range(len(batch_sizes)), batch_sizes)
  plt.xlabel('Batch number')
  plt.ylabel('Batch size')
  plt.title(name)
  
#plot_batch_sizes(X_Test_data,"Test Dataset")  

# Pre-processing 
def norm(x,_min,_max):
   return (x - _min)/(_max - _min)

def normalize_samples(feature,label):
   X = norm(feature,0,255)
   Y = label
   return X,Y
##Using the dataset batch function, shuffle and divide the dataset into batches
X_Test_data_norm = X_Test_data.map(normalize_samples)

# Batch split 
X_Test_data_norm = X_Test_data_norm.batch(125)


"""######################## Evaluation ######################"""
# Getting predicted values on test dataset from model
print("prediction starts")
result = model.predict(X_Test_data_norm,verbose=1)
y_pred = [numpy.argmax(i) for i in result]
y_pred = numpy.asarray(y_pred)
print(y_pred)
# #calculate the true label for test dataset
print("y_true extraction starts")
y_true = []
for i in X_Test_data_norm.unbatch():
    y_true.append(numpy.argmax(i[1].numpy()))
y_true = numpy.asarray(y_true)
print(y_true)

# # find the confusion matrix
print("result calculation starts")
acc = accuracy_score(y_pred,y_true)
cm = confusion_matrix(y_true, y_pred)
temp = precision_recall_fscore_support(y_true, y_pred, average='weighted')
precision = temp[0]
recall = temp[1]
F1score = temp[2]


test_loss,test_accuracy  = model.evaluate(X_Test_data_norm,verbose = 1)
print(" ")
print("On Test Set: (loss,acc): %.5f %.5f" % (test_loss,test_accuracy))
# #F1score = f1_score(y_true,y_pred)
print("sklearn accuracy : {}".format(acc))
print("Precision: ", precision)
print("Recall: ", recall)
print("F1_Score: ", F1score)
print("confusion matrix:")
print("")
print(cm)

